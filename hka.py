import streamlit as st
import pandas as pd
import requests
import time
import random
import plotly.express as px
import datetime
from bs4 import BeautifulSoup
from collections import Counter
import jieba.analyse
import platform

# --- æ–°å¢ï¼šè‡ªåŠ¨åŒ–ç™»å½•æ¨¡å— (å¤šæµè§ˆå™¨æ”¯æŒ) ---
from selenium import webdriver
# Chrome
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
# Edge
from selenium.webdriver.edge.service import Service as EdgeService
from webdriver_manager.microsoft import EdgeChromiumDriverManager
# Firefox
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="WeChat Insight Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ ¸å¿ƒçˆ¬è™«é€»è¾‘ (ä¿æŒä¸å˜) ---
class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search_account(self, query):
        """æœç´¢å…¬ä¼—å·è·å–fakeid"""
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params)
            data = res.json()
            return data.get("list", [])
        except Exception as e:
            st.error(f"æœç´¢è¯·æ±‚å¼‚å¸¸: {e}")
            return []

    def fetch_article_list(self, fakeid, pages=3):
        """è·å–æ–‡ç« åˆ—è¡¨å…ƒæ•°æ®"""
        all_articles = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for page in range(pages):
            status_text.text(f"ğŸ“¡ æ­£åœ¨æ‰«æåˆ—è¡¨ç¬¬ {page + 1}/{pages} é¡µ...")
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "item_idx": item.get("item_idx", 1),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                progress_bar.progress((page + 1) / pages)
                time.sleep(random.uniform(1.0, 2.0))
            except:
                break
        
        progress_bar.empty()
        status_text.empty()
        return all_articles

    def fetch_article_content(self, url):
        """æ·±åº¦é‡‡é›†ï¼šè®¿é—®è¯¦æƒ…é¡µè·å–æ­£æ–‡"""
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, "lxml")
            
            content_div = soup.find("div", {"id": "js_content"})
            if content_div:
                for p in content_div.find_all('p'):
                    p.insert_after('\n')
                content_text = content_div.get_text().strip()
            else:
                content_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"})
            if not author_tag:
                author_tag = soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            scripts = soup.find_all("script")
            ip_location = "IPæœªçŸ¥"
            for script in scripts:
                if script.string and "ip_wording" in script.string:
                    import re
                    match = re.search(r'ip_wording\s*=\s*\{\s*type\s*:\s*2\s*,\s*name\s*:\s*"(.*?)"', script.string)
                    if match:
                        ip_location = match.group(1)
                        break
            
            return content_text, author, ip_location
        except Exception:
            return "", "è·å–å¤±è´¥", "è·å–å¤±è´¥"

# --- æ•°æ®å¤„ç†å·¥å…· ---
def process_data(articles, crawler=None, fetch_details=False):
    if not articles:
        return pd.DataFrame()
    
    df = pd.DataFrame(articles)
    df['publish_time'] = pd.to_datetime(df['create_time'], unit='s')
    df['date'] = df['publish_time'].dt.date
    df['is_original'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
    
    if fetch_details and crawler:
        st.info("ğŸ¢ æ­£åœ¨æ·±åº¦é‡‡é›†å…¨æ–‡ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        details = []
        bar = st.progress(0)
        for idx, row in df.iterrows():
            content, author, ip = crawler.fetch_article_content(row['link'])
            details.append({'content': content, 'author': author, 'ip_location': ip})
            bar.progress((idx + 1) / len(df))
            time.sleep(0.5)
        
        detail_df = pd.DataFrame(details)
        df = pd.concat([df, detail_df], axis=1)
        bar.empty()
    else:
        df['content'] = ""
        df['author'] = "æœªé‡‡é›†"
        df['ip_location'] = "-"

    return df

# --- è¾…åŠ©å‡½æ•°ï¼šè‡ªåŠ¨ç™»å½•è·å–Cookie (å¤šæµè§ˆå™¨ç‰ˆ) ---
def auto_login_get_cookie(browser_type="Chrome"):
    driver = None
    status_placeholder = st.empty()
    
    try:
        status_placeholder.info(f"ğŸš€ æ­£åœ¨å¯åŠ¨ {browser_type} æµè§ˆå™¨ï¼Œè¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­æ‰«ç ç™»å½•...")
        
        # æ ¹æ®é€‰æ‹©åˆå§‹åŒ–ä¸åŒçš„æµè§ˆå™¨é©±åŠ¨
        if browser_type == "Chrome":
            options = webdriver.ChromeOptions()
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            
        elif browser_type == "Edge":
            options = webdriver.EdgeOptions()
            service = EdgeService(EdgeChromiumDriverManager().install())
            driver = webdriver.Edge(service=service, options=options)
            
        elif browser_type == "Firefox":
            options = webdriver.FirefoxOptions()
            service = FirefoxService(GeckoDriverManager().install())
            driver = webdriver.Firefox(service=service, options=options)
            
        elif browser_type == "Safari":
            if platform.system() != 'Darwin':
                st.error("Safari æµè§ˆå™¨ä»…æ”¯æŒ macOS ç³»ç»Ÿã€‚")
                return None, None
            # Safari ä¸éœ€è¦ä¸‹è½½é©±åŠ¨ï¼Œæ˜¯ç³»ç»Ÿå†…ç½®çš„
            # æ³¨æ„ï¼šéœ€åœ¨ Safari èœå• -> å¼€å‘ -> å…è®¸è¿œç¨‹è‡ªåŠ¨åŒ– (Allow Remote Automation)
            options = webdriver.SafariOptions()
            driver = webdriver.Safari(options=options)
            
        # æ‰“å¼€å¾®ä¿¡å…¬ä¼—å¹³å°
        driver.get("https://mp.weixin.qq.com/")
        
        # å¾ªç¯æ£€æµ‹æ˜¯å¦ç™»å½•æˆåŠŸ
        max_wait = 180
        start_time = time.time()
        
        token = ""
        cookies_str = ""
        
        while True:
            # æ•è·æµè§ˆå™¨å¯èƒ½è¢«æ‰‹åŠ¨å…³é—­çš„å¼‚å¸¸
            try:
                current_url = driver.current_url
            except:
                status_placeholder.warning("æµè§ˆå™¨å·²å…³é—­ï¼Œæ“ä½œç»ˆæ­¢ã€‚")
                return None, None

            if "token=" in current_url:
                status_placeholder.success("âœ… æ‰«ç æˆåŠŸï¼æ­£åœ¨æå–å‡­è¯...")
                try:
                    token = current_url.split("token=")[1].split("&")[0]
                except:
                    pass
                
                selenium_cookies = driver.get_cookies()
                cookie_items = [f"{c['name']}={c['value']}" for c in selenium_cookies]
                cookies_str = "; ".join(cookie_items)
                break
            
            if time.time() - start_time > max_wait:
                status_placeholder.error("â° ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                break
            
            time.sleep(1)
        
        driver.quit()
        status_placeholder.empty()
        return token, cookies_str
        
    except Exception as e:
        error_msg = str(e)
        if browser_type == "Safari" and "Could not create a session" in error_msg:
             st.error("å¯åŠ¨ Safari å¤±è´¥ã€‚è¯·ç¡®ä¿å·²åœ¨ Safari èœå•æ ä¸­å¼€å¯ 'å¼€å‘' -> 'å…è®¸è¿œç¨‹è‡ªåŠ¨åŒ–'ã€‚")
        else:
             st.error(f"å¯åŠ¨ {browser_type} æµè§ˆå™¨å¤±è´¥: {error_msg}")
        
        if driver:
            try:
                driver.quit()
            except:
                pass
        return None, None

# --- ä¸»ç¨‹åºé€»è¾‘ ---

if 'wx_token' not in st.session_state:
    st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state:
    st.session_state['wx_cookie'] = ''

with st.sidebar:
    st.title("ğŸ¤– è‡ªåŠ¨è·å–åŠ©æ‰‹")
    
    # æµè§ˆå™¨é€‰æ‹©
    browser_choice = st.selectbox(
        "é€‰æ‹©æµè§ˆå™¨", 
        ["Chrome", "Edge", "Safari", "Firefox"],
        help="Safari éœ€åœ¨èœå•æ å¼€å¯'å…è®¸è¿œç¨‹è‡ªåŠ¨åŒ–'ï¼›å…¶ä»–æµè§ˆå™¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½é©±åŠ¨ã€‚"
    )
    
    # è‡ªåŠ¨è·å–æŒ‰é’®
    if st.button("ğŸ“¢ å”¤èµ·æµè§ˆå™¨æ‰«ç ", type="primary"):
        token, cookie = auto_login_get_cookie(browser_choice)
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.success("å‡­è¯å·²è‡ªåŠ¨å¡«å…¥ï¼")
            time.sleep(1)
            st.rerun()
    
    st.divider()
    
    with st.expander("ğŸ”‘ å‡­è¯é…ç½®", expanded=True):
        wx_token = st.text_input("Token", value=st.session_state['wx_token'])
        wx_cookie = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=150)
    
    st.divider()
    target_query = st.text_input("ğŸ” ç›®æ ‡å…¬ä¼—å·", placeholder="è¾“å…¥åç§°")
    scrape_pages = st.number_input("æŠ“å–é¡µæ•°", 1, 10, 2)
    enable_details = st.checkbox("é‡‡é›†æ­£æ–‡ (é˜…è¯»æ¨¡å¼å¿…é€‰)", value=True)
    
    start_btn = st.button("ğŸš€ å¼€å§‹åˆ†ææ•°æ®", use_container_width=True)

# --- ä¸»ç•Œé¢ (ä¿æŒä¸å˜) ---
if start_btn and wx_token and wx_cookie and target_query:
    crawler = WechatCrawler(wx_token, wx_cookie)
    
    with st.status("æ­£åœ¨å»ºç«‹æ•°æ®è¿æ¥...", expanded=True) as status:
        status.write("ğŸ” å®šä½ç›®æ ‡è´¦å·...")
        accounts = crawler.search_account(target_query)
        if not accounts:
            status.update(label="æœªæ‰¾åˆ°è´¦å·ï¼Œå¯èƒ½æ˜¯Cookieå·²å¤±æ•ˆï¼Œè¯·é‡æ–°æ‰«ç ", state="error")
            st.stop()
        
        target = accounts[0]
        status.write(f"âœ… é”å®š: {target['nickname']}")
        
        status.write("ğŸ“ƒ æ‹‰å–æ–‡ç« åˆ—è¡¨...")
        raw_list = crawler.fetch_article_list(target['fakeid'], pages=scrape_pages)
        
        if not raw_list:
             status.update(label="æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥å‡­è¯", state="error")
             st.stop()

        status.write("ğŸ§¹ æ·±åº¦é‡‡é›†æ­£æ–‡å†…å®¹...")
        df_res = process_data(raw_list, crawler, fetch_details=enable_details)
        
        status.update(label="æ•°æ®å‡†å¤‡å°±ç»ª!", state="complete")
        
        st.session_state['data'] = df_res
        st.session_state['account'] = target['nickname']

if 'data' in st.session_state:
    df = st.session_state['data']
    nickname = st.session_state['account']
    st.header(f"ğŸ“° {nickname} Â· æ·±åº¦é˜…è¯»çœ‹æ¿")
    
    tab_read, tab_list = st.tabs(["ğŸ‘“ é˜…è¯»æ¨¡å¼", "ğŸ“‹ æ–‡ç« åˆ—è¡¨"])
    
    with tab_read:
        if 'content' in df.columns and not df['content'].isna().all():
            df['select_label'] = df['date'].astype(str) + " | " + df['title']
            selected_article_label = st.selectbox("é€‰æ‹©æ–‡ç« :", df['select_label'].tolist())
            article = df[df['select_label'] == selected_article_label].iloc[0]
            
            with st.container():
                st.markdown(f"## {article['title']}")
                st.caption(f"ä½œè€…: {article['author']} | å‘å¸ƒæ—¶é—´: {article['publish_time']} | {article['is_original']} | IP: {article['ip_location']}")
                st.divider()
                if article['content']:
                    st.markdown(article['content'].replace("\n", "\n\n"))
                else:
                    st.warning("æ­£æ–‡å†…å®¹ä¸ºç©ºæˆ–æœªé‡‡é›†")
                    st.markdown(f"[ç‚¹å‡»è·³è½¬åŸæ–‡é“¾æ¥]({article['link']})")
        else:
            st.info("æš‚æ— æ­£æ–‡æ•°æ®ï¼Œè¯·ç¡®ä¿å‹¾é€‰äº†ã€é‡‡é›†æ­£æ–‡ã€‘å¹¶é‡æ–°æŠ“å–ã€‚")
            
    with tab_list:
        st.dataframe(
            df[['title', 'date', 'author', 'is_original', 'link']],
            use_container_width=True,
            column_config={"link": st.column_config.LinkColumn("åŸæ–‡é“¾æ¥")}
        )
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é€‰æ‹©æµè§ˆå™¨å¹¶ç‚¹å‡» **'å”¤èµ·æµè§ˆå™¨æ‰«ç '**ã€‚")

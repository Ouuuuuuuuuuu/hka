import streamlit as st
import pandas as pd
import requests
import time
import random
import plotly.express as px
import datetime
from bs4 import BeautifulSoup
from collections import Counter
import jieba.analyse

# --- æ–°å¢ï¼šè‡ªåŠ¨åŒ–ç™»å½•æ¨¡å— ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="WeChat Insight Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ ¸å¿ƒçˆ¬è™«é€»è¾‘ ---
class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search_account(self, query):
        """æœç´¢å…¬ä¼—å·è·å–fakeid"""
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params)
            data = res.json()
            return data.get("list", [])
        except Exception as e:
            st.error(f"æœç´¢è¯·æ±‚å¼‚å¸¸: {e}")
            return []

    def fetch_article_list(self, fakeid, pages=3):
        """è·å–æ–‡ç« åˆ—è¡¨å…ƒæ•°æ®"""
        all_articles = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for page in range(pages):
            status_text.text(f"ğŸ“¡ æ­£åœ¨æ‰«æåˆ—è¡¨ç¬¬ {page + 1}/{pages} é¡µ...")
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "item_idx": item.get("item_idx", 1),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                progress_bar.progress((page + 1) / pages)
                # éšæœºå»¶æ—¶ï¼Œé¿å…è§¦å‘åçˆ¬æœºåˆ¶
                time.sleep(random.uniform(1.0, 2.0))
            except:
                break
        
        progress_bar.empty()
        status_text.empty()
        return all_articles

    def fetch_article_content(self, url):
        """æ·±åº¦é‡‡é›†ï¼šè®¿é—®è¯¦æƒ…é¡µè·å–æ­£æ–‡ã€ä½œè€…ç­‰ä¿¡æ¯"""
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, "lxml")
            
            # æå–æ­£æ–‡æ–‡æœ¬ (å»é™¤HTMLæ ‡ç­¾ï¼Œä¿ç•™æ®µè½ç»“æ„)
            content_div = soup.find("div", {"id": "js_content"})
            if content_div:
                # ç®€å•å¤„ç†ï¼šå°†pæ ‡ç­¾æ¢è¡Œï¼Œå¢å¼ºé˜…è¯»ä½“éªŒ
                for p in content_div.find_all('p'):
                    p.insert_after('\n')
                content_text = content_div.get_text().strip()
            else:
                content_text = ""
            
            # æå–ä½œè€…
            author_tag = soup.find("strong", {"class": "profile_nickname"})
            if not author_tag:
                author_tag = soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            # å°è¯•æå–IPå±åœ° (å¾®ä¿¡IPå±åœ°é€šå¸¸åœ¨Scriptå˜é‡ä¸­)
            scripts = soup.find_all("script")
            ip_location = "IPæœªçŸ¥"
            for script in scripts:
                if script.string and "ip_wording" in script.string:
                    import re
                    match = re.search(r'ip_wording\s*=\s*\{\s*type\s*:\s*2\s*,\s*name\s*:\s*"(.*?)"', script.string)
                    if match:
                        ip_location = match.group(1)
                        break
            
            return content_text, author, ip_location
        except Exception:
            return "", "è·å–å¤±è´¥", "è·å–å¤±è´¥"

# --- æ•°æ®å¤„ç†å·¥å…· ---
def process_data(articles, crawler=None, fetch_details=False):
    if not articles:
        return pd.DataFrame()
    
    df = pd.DataFrame(articles)
    
    # æ—¶é—´æ ¼å¼åŒ–
    df['publish_time'] = pd.to_datetime(df['create_time'], unit='s')
    df['date'] = df['publish_time'].dt.date
    df['is_original'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
    
    # æ·±åº¦é‡‡é›†é€»è¾‘
    if fetch_details and crawler:
        st.info("ğŸ¢ æ­£åœ¨æ·±åº¦é‡‡é›†å…¨æ–‡ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        details = []
        bar = st.progress(0)
        for idx, row in df.iterrows():
            content, author, ip = crawler.fetch_article_content(row['link'])
            details.append({
                'content': content,
                'author': author,
                'ip_location': ip
            })
            bar.progress((idx + 1) / len(df))
            time.sleep(0.5) # å¿…é¡»å»¶æ—¶ï¼Œå¦åˆ™å®¹æ˜“å°IP
        
        detail_df = pd.DataFrame(details)
        df = pd.concat([df, detail_df], axis=1)
        bar.empty()
    else:
        df['content'] = ""
        df['author'] = "æœªé‡‡é›†"
        df['ip_location'] = "-"

    return df

# --- è¾…åŠ©å‡½æ•°ï¼šè‡ªåŠ¨ç™»å½•è·å–Cookie ---
def auto_login_get_cookie():
    try:
        # åˆå§‹åŒ– Chrome é©±åŠ¨
        options = webdriver.ChromeOptions()
        # æ³¨æ„ï¼šæ‰«ç ç™»å½•ä¸èƒ½ä½¿ç”¨æ— å¤´æ¨¡å¼(headless)ï¼Œå¿…é¡»æ˜¾ç¤ºæµè§ˆå™¨çª—å£
        
        status_placeholder = st.empty()
        status_placeholder.info("ğŸš€ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨ï¼Œè¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­æ‰«ç ç™»å½•...")
        
        # è‡ªåŠ¨ä¸‹è½½å¹¶å¯åŠ¨åŒ¹é…ç‰ˆæœ¬çš„ ChromeDriver
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        
        # æ‰“å¼€å¾®ä¿¡å…¬ä¼—å¹³å°
        driver.get("https://mp.weixin.qq.com/")
        
        # å¾ªç¯æ£€æµ‹æ˜¯å¦ç™»å½•æˆåŠŸ (æ£€æµ‹URLä¸­æ˜¯å¦åŒ…å« token)
        max_wait = 180 # 3åˆ†é’Ÿè¶…æ—¶
        start_time = time.time()
        
        token = ""
        cookies_str = ""
        
        while True:
            current_url = driver.current_url
            if "token=" in current_url:
                status_placeholder.success("âœ… æ‰«ç æˆåŠŸï¼æ­£åœ¨æå–å‡­è¯...")
                # æå– Token
                try:
                    token = current_url.split("token=")[1].split("&")[0]
                except:
                    pass
                
                # æå– Cookie
                selenium_cookies = driver.get_cookies()
                # å°† Cookie åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                cookie_items = [f"{c['name']}={c['value']}" for c in selenium_cookies]
                cookies_str = "; ".join(cookie_items)
                
                break
            
            if time.time() - start_time > max_wait:
                status_placeholder.error("â° ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                break
            
            time.sleep(1)
        
        driver.quit()
        status_placeholder.empty()
        return token, cookies_str
        
    except Exception as e:
        st.error(f"å¯åŠ¨æµè§ˆå™¨å¤±è´¥ï¼Œè¯·ç¡®ä¿å®‰è£…äº† Chrome æµè§ˆå™¨: {str(e)}")
        return None, None

# --- ä¸»ç¨‹åºé€»è¾‘ ---

# åˆå§‹åŒ– Session State ç”¨äºå­˜å‚¨ Token å’Œ Cookie
if 'wx_token' not in st.session_state:
    st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state:
    st.session_state['wx_cookie'] = ''

with st.sidebar:
    st.title("ğŸ¤– è‡ªåŠ¨è·å–åŠ©æ‰‹")
    
    # è‡ªåŠ¨è·å–æŒ‰é’®
    if st.button("ğŸ“¢ ç‚¹å‡»å”¤èµ·æµè§ˆå™¨æ‰«ç ", type="primary"):
        token, cookie = auto_login_get_cookie()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.success("å‡­è¯å·²è‡ªåŠ¨å¡«å…¥ï¼")
            time.sleep(1)
            st.rerun() # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºå¡«å…¥çš„æ•°æ®
    
    st.divider()
    
    with st.expander("ğŸ”‘ å‡­è¯é…ç½®", expanded=True):
        # ä½¿ç”¨ session_state è‡ªåŠ¨å¡«å……
        wx_token = st.text_input("Token", value=st.session_state['wx_token'], help="URLä¸­çš„tokenå‚æ•°")
        wx_cookie = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=150, help="å®Œæ•´çš„Cookieå­—ç¬¦ä¸²")
    
    st.divider()
    target_query = st.text_input("ğŸ” ç›®æ ‡å…¬ä¼—å·", placeholder="è¾“å…¥åç§°ï¼Œå¦‚ï¼šæ¸…åå¤§å­¦")
    scrape_pages = st.number_input("æŠ“å–é¡µæ•°", 1, 10, 2)
    enable_details = st.checkbox("é‡‡é›†æ­£æ–‡ (é˜…è¯»æ¨¡å¼å¿…é€‰)", value=True)
    
    start_btn = st.button("ğŸš€ å¼€å§‹åˆ†ææ•°æ®", use_container_width=True)

# --- ä¸»ç•Œé¢ ---

if start_btn and wx_token and wx_cookie and target_query:
    crawler = WechatCrawler(wx_token, wx_cookie)
    
    with st.status("æ­£åœ¨å»ºç«‹æ•°æ®è¿æ¥...", expanded=True) as status:
        # 1. æœç´¢
        status.write("ğŸ” å®šä½ç›®æ ‡è´¦å·...")
        accounts = crawler.search_account(target_query)
        if not accounts:
            status.update(label="æœªæ‰¾åˆ°è´¦å·ï¼Œå¯èƒ½æ˜¯Cookieå·²å¤±æ•ˆï¼Œè¯·é‡æ–°æ‰«ç ", state="error")
            st.stop()
        
        target = accounts[0]
        status.write(f"âœ… é”å®š: {target['nickname']}")
        
        # 2. åˆ—è¡¨æŠ“å–
        status.write("ğŸ“ƒ æ‹‰å–æ–‡ç« åˆ—è¡¨...")
        raw_list = crawler.fetch_article_list(target['fakeid'], pages=scrape_pages)
        
        if not raw_list:
             status.update(label="æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥å‡­è¯", state="error")
             st.stop()

        # 3. æ·±åº¦é‡‡é›†
        status.write("ğŸ§¹ æ·±åº¦é‡‡é›†æ­£æ–‡å†…å®¹...")
        df_res = process_data(raw_list, crawler, fetch_details=enable_details)
        
        status.update(label="æ•°æ®å‡†å¤‡å°±ç»ª!", state="complete")
        
        st.session_state['data'] = df_res
        st.session_state['account'] = target['nickname']

# --- çœ‹æ¿å±•ç¤º ---
if 'data' in st.session_state:
    df = st.session_state['data']
    nickname = st.session_state['account']
    st.header(f"ğŸ“° {nickname} Â· æ·±åº¦é˜…è¯»çœ‹æ¿")
    
    tab_read, tab_list = st.tabs(["ğŸ‘“ é˜…è¯»æ¨¡å¼", "ğŸ“‹ æ–‡ç« åˆ—è¡¨"])
    
    with tab_read:
        if 'content' in df.columns and not df['content'].isna().all():
            # ç”Ÿæˆä¸‹æ‹‰æ¡†é€‰é¡¹
            df['select_label'] = df['date'].astype(str) + " | " + df['title']
            selected_article_label = st.selectbox("é€‰æ‹©æ–‡ç« :", df['select_label'].tolist())
            
            # è·å–é€‰ä¸­æ–‡ç« æ•°æ®
            article = df[df['select_label'] == selected_article_label].iloc[0]
            
            # æ–‡ç« å±•ç¤ºå®¹å™¨
            with st.container():
                st.markdown(f"## {article['title']}")
                st.caption(f"ä½œè€…: {article['author']} | å‘å¸ƒæ—¶é—´: {article['publish_time']} | {article['is_original']} | IP: {article['ip_location']}")
                st.divider()
                
                # æ­£æ–‡å±•ç¤ºåŒº
                if article['content']:
                    st.markdown(article['content'].replace("\n", "\n\n")) # ä¼˜åŒ–æ’ç‰ˆ
                else:
                    st.warning("æ­£æ–‡å†…å®¹ä¸ºç©ºæˆ–æœªé‡‡é›†")
                    st.markdown(f"[ç‚¹å‡»è·³è½¬åŸæ–‡é“¾æ¥]({article['link']})")
        else:
            st.info("æš‚æ— æ­£æ–‡æ•°æ®ï¼Œè¯·ç¡®ä¿å‹¾é€‰äº†ã€é‡‡é›†æ­£æ–‡ã€‘å¹¶é‡æ–°æŠ“å–ã€‚")
            
    with tab_list:
        st.dataframe(
            df[['title', 'date', 'author', 'is_original', 'link']],
            use_container_width=True,
            column_config={
                "link": st.column_config.LinkColumn("åŸæ–‡é“¾æ¥")
            }
        )
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ç‚¹å‡» **'å”¤èµ·æµè§ˆå™¨æ‰«ç '** è·å–å‡­è¯ï¼Œç„¶åè¾“å…¥å…¬ä¼—å·åç§°å¼€å§‹æŠ“å–ã€‚")

import streamlit as st
import pandas as pd
import requests
import time
import random
import plotly.express as px
import datetime
from bs4 import BeautifulSoup
from collections import Counter
import jieba.analyse
import platform
import os
import shutil
import glob

# --- Selenium ç›¸å…³åº“ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.firefox.service import Service as FirefoxService

# å¼•å…¥ webdriver_manager
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.microsoft import EdgeChromiumDriverManager
from webdriver_manager.firefox import GeckoDriverManager

# ==========================================
# ğŸš€ é…ç½®å›½å†…é•œåƒæº
# ==========================================
os.environ['WDM_BASE_URL'] = "https://npmmirror.com/mirrors/chromedriver"
os.environ['WDM_SSL_VERIFY'] = '0' 

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="WeChat Insight Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- è¾…åŠ©å‡½æ•°ï¼šæ™ºèƒ½æŸ¥æ‰¾æœ¬åœ°é©±åŠ¨ ---
def find_local_driver(browser_name):
    """
    å…¨ç›˜æ‰«æï¼šåœ¨ Downloads æ–‡ä»¶å¤¹å’Œç³»ç»Ÿè·¯å¾„ä¸­æŸ¥æ‰¾é©±åŠ¨æ–‡ä»¶
    """
    system_name = platform.system()
    driver_filename = "chromedriver" if browser_name == "Chrome" else "msedgedriver"
    if system_name == "Windows":
        driver_filename += ".exe"

    # 1. æ£€æŸ¥å½“å‰ç›®å½•
    if os.path.exists(driver_filename):
        return os.path.abspath(driver_filename)
    
    # 2. æ£€æŸ¥ Downloads ç›®å½•
    home = os.path.expanduser("~")
    downloads_path = os.path.join(home, "Downloads")
    target = os.path.join(downloads_path, driver_filename)
    if os.path.exists(target):
        return target
        
    # 3. æ£€æŸ¥ç³»ç»Ÿ PATH
    return shutil.which(driver_filename)

# --- æ ¸å¿ƒé€»è¾‘ï¼šåˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨ ---
# å°†æ­¤é€»è¾‘ç‹¬ç«‹å‡ºæ¥ï¼Œé¿å…ä¸»å‡½æ•°å‡ºç° SyntaxError
def init_driver_engine(browser_type):
    driver = None
    err_msg = ""
    
    try:
        # === Safari ç­–ç•¥ (Macä¸“ç”¨) ===
        if browser_type == "Safari":
            if platform.system() != 'Darwin':
                return None, "Safari ä»…æ”¯æŒ macOS ç³»ç»Ÿã€‚"
            try:
                options = webdriver.SafariOptions()
                driver = webdriver.Safari(options=options)
                return driver, ""
            except Exception as e:
                return None, f"Safari å¯åŠ¨å¤±è´¥: {str(e)}ã€‚è¯·ç¡®ä¿åœ¨ Safari èœå•æ  -> å¼€å‘ -> å‹¾é€‰ 'å…è®¸è¿œç¨‹è‡ªåŠ¨åŒ–'ã€‚"

        # === Chrome ç­–ç•¥ ===
        elif browser_type == "Chrome":
            options = webdriver.ChromeOptions()
            # ç­–ç•¥A: è‡ªåŠ¨ä¸‹è½½ (å›½å†…é•œåƒ)
            try:
                service = ChromeService(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=options)
                return driver, ""
            except Exception:
                # ç­–ç•¥B: æŸ¥æ‰¾æœ¬åœ°
                local_path = find_local_driver("Chrome")
                if local_path:
                    st.toast(f"å·²è°ƒç”¨æœ¬åœ°é©±åŠ¨: {local_path}", icon="ğŸ“‚")
                    service = ChromeService(executable_path=local_path)
                    driver = webdriver.Chrome(service=service, options=options)
                    return driver, ""
                else:
                    return None, "Chrome é©±åŠ¨ä¸‹è½½å¤±è´¥ä¸”æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ã€‚"

        # === Edge ç­–ç•¥ ===
        elif browser_type == "Edge":
            options = webdriver.EdgeOptions()
            # ç­–ç•¥A: æŸ¥æ‰¾æœ¬åœ° (ä¼˜å…ˆ)
            local_path = find_local_driver("Edge")
            if local_path:
                st.toast(f"å·²è°ƒç”¨æœ¬åœ°é©±åŠ¨: {local_path}", icon="ğŸ“‚")
                try:
                    service = EdgeService(executable_path=local_path)
                    driver = webdriver.Edge(service=service, options=options)
                    return driver, ""
                except Exception as e:
                    # å¦‚æœæœ¬åœ°é©±åŠ¨ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½
                    pass 

            # ç­–ç•¥B: è‡ªåŠ¨ä¸‹è½½
            try:
                service = EdgeService(EdgeChromiumDriverManager().install())
                driver = webdriver.Edge(service=service, options=options)
                return driver, ""
            except Exception as e:
                return None, f"Edge é©±åŠ¨å¯åŠ¨å¤±è´¥: {str(e)}ã€‚è¯·æ‰‹åŠ¨ä¸‹è½½é©±åŠ¨æ”¾å…¥ Downloads æ–‡ä»¶å¤¹ã€‚"

    except Exception as e:
        return None, f"æœªçŸ¥é”™è¯¯: {str(e)}"
    
    return None, "ä¸æ”¯æŒçš„æµè§ˆå™¨ç±»å‹"

# --- æ ¸å¿ƒçˆ¬è™«é€»è¾‘ç±» ---
class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search_account(self, query):
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params)
            data = res.json()
            return data.get("list", [])
        except Exception as e:
            return []

    def fetch_article_list(self, fakeid, pages=3):
        all_articles = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for page in range(pages):
            status_text.text(f"ğŸ“¡ æ­£åœ¨æ‰«æåˆ—è¡¨ç¬¬ {page + 1}/{pages} é¡µ...")
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "item_idx": item.get("item_idx", 1),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                progress_bar.progress((page + 1) / pages)
                time.sleep(random.uniform(1.0, 2.0))
            except:
                break
        
        progress_bar.empty()
        status_text.empty()
        return all_articles

    def fetch_article_content(self, url):
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, "lxml")
            
            content_div = soup.find("div", {"id": "js_content"})
            if content_div:
                for p in content_div.find_all('p'):
                    p.insert_after('\n')
                content_text = content_div.get_text().strip()
            else:
                content_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"})
            if not author_tag:
                author_tag = soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return content_text, author, "IPæœªçŸ¥"
        except Exception:
            return "", "è·å–å¤±è´¥", "è·å–å¤±è´¥"

# --- æ•°æ®å¤„ç†å·¥å…· ---
def process_data(articles, crawler=None, fetch_details=False):
    if not articles:
        return pd.DataFrame()
    df = pd.DataFrame(articles)
    df['publish_time'] = pd.to_datetime(df['create_time'], unit='s')
    df['date'] = df['publish_time'].dt.date
    df['is_original'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
    
    if fetch_details and crawler:
        st.info("ğŸ¢ æ­£åœ¨æ·±åº¦é‡‡é›†å…¨æ–‡...")
        details = []
        bar = st.progress(0)
        for idx, row in df.iterrows():
            content, author, ip = crawler.fetch_article_content(row['link'])
            details.append({'content': content, 'author': author, 'ip_location': ip})
            bar.progress((idx + 1) / len(df))
            time.sleep(0.5)
        detail_df = pd.DataFrame(details)
        df = pd.concat([df, detail_df], axis=1)
        bar.empty()
    else:
        df['content'] = ""
        df['author'] = "æœªé‡‡é›†"
    return df

# --- ä¸»äº¤äº’å‡½æ•°ï¼šæ‰«ç è·å–å‡­è¯ ---
def auto_login_get_cookie(browser_type):
    status_placeholder = st.empty()
    status_placeholder.info(f"ğŸš€ æ­£åœ¨å¯åŠ¨ {browser_type}ï¼Œè¯·ç¨å€™...")
    
    # 1. å¯åŠ¨æµè§ˆå™¨
    driver, error = init_driver_engine(browser_type)
    
    if not driver:
        status_placeholder.error(error)
        return None, None
    
    try:
        # 2. æ‰“å¼€å¾®ä¿¡
        driver.get("https://mp.weixin.qq.com/")
        status_placeholder.success("âœ… æµè§ˆå™¨å·²å°±ç»ªï¼è¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­æ‰«ç ç™»å½•...")
        
        # 3. å¾ªç¯æ£€æµ‹ç™»å½•
        max_wait = 180
        start_time = time.time()
        
        while True:
            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > max_wait:
                status_placeholder.error("â° ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                break
                
            # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦è¢«ç”¨æˆ·å…³é—­
            try:
                current_url = driver.current_url
            except:
                status_placeholder.warning("âš ï¸ æµè§ˆå™¨å·²å…³é—­")
                return None, None

            # æ£€æŸ¥æ˜¯å¦åŒ…å« token (ç™»å½•æˆåŠŸæ ‡å¿—)
            if "token=" in current_url:
                status_placeholder.success("ğŸ‰ æ‰«ç æˆåŠŸï¼æ­£åœ¨æå–å‡­è¯...")
                try:
                    token = current_url.split("token=")[1].split("&")[0]
                except:
                    token = ""
                
                selenium_cookies = driver.get_cookies()
                cookie_items = [f"{c['name']}={c['value']}" for c in selenium_cookies]
                cookies_str = "; ".join(cookie_items)
                
                driver.quit()
                return token, cookies_str
            
            time.sleep(1)
            
    except Exception as e:
        status_placeholder.error(f"è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        if driver:
            try: driver.quit() 
            except: pass
        return None, None
        
    return None, None

# --- ä¸»ç¨‹åº UI é€»è¾‘ ---

if 'wx_token' not in st.session_state:
    st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state:
    st.session_state['wx_cookie'] = ''

with st.sidebar:
    st.title("ğŸ¤– è‡ªåŠ¨è·å–åŠ©æ‰‹")
    
    # æ™ºèƒ½é»˜è®¤é€‰æ‹©
    default_idx = 0 if platform.system() == 'Darwin' else 2 # Macé»˜è®¤Safari, Winé»˜è®¤Edge
    browser_choice = st.selectbox("é€‰æ‹©æµè§ˆå™¨", ["Safari", "Chrome", "Edge"], index=default_idx)
    
    if browser_choice == "Safari":
        st.caption("ğŸ **Macé¦–é€‰**ï¼šæ— éœ€ä¸‹è½½é©±åŠ¨ã€‚è‹¥å¤±è´¥è¯·æ£€æŸ¥Safarièœå•æ  `å¼€å‘` -> `å…è®¸è¿œç¨‹è‡ªåŠ¨åŒ–`ã€‚")
    elif browser_choice == "Edge":
        st.caption("âš¡ï¸ **è‡ªåŠ¨æœç´¢**ï¼šå°†ä¸‹è½½å¥½çš„é©±åŠ¨æ”¾åœ¨ Downloads æ–‡ä»¶å¤¹ï¼Œæˆ‘ä¼šè‡ªåŠ¨æ‰¾åˆ°å®ƒã€‚")

    if st.button("ğŸ“¢ ä¸€é”®å”¤èµ·æ‰«ç ", type="primary"):
        token, cookie = auto_login_get_cookie(browser_choice)
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.balloons()
            st.success("å‡­è¯å·²è‡ªåŠ¨å¡«å…¥ï¼")
            time.sleep(1)
            st.rerun()
    
    st.divider()
    
    with st.expander("ğŸ”‘ å‡­è¯é…ç½® (æ‰‹åŠ¨)", expanded=True):
        wx_token = st.text_input("Token", value=st.session_state['wx_token'])
        wx_cookie = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=150)
    
    st.divider()
    target_query = st.text_input("ğŸ” ç›®æ ‡å…¬ä¼—å·", placeholder="è¾“å…¥åç§°")
    scrape_pages = st.number_input("æŠ“å–é¡µæ•°", 1, 10, 2)
    enable_details = st.checkbox("é‡‡é›†æ­£æ–‡ (é˜…è¯»æ¨¡å¼å¿…é€‰)", value=True)
    
    start_btn = st.button("ğŸš€ å¼€å§‹åˆ†ææ•°æ®", use_container_width=True)

# --- ä¸»ç•Œé¢ ---
if start_btn and wx_token and wx_cookie and target_query:
    crawler = WechatCrawler(wx_token, wx_cookie)
    
    with st.status("æ­£åœ¨å»ºç«‹æ•°æ®è¿æ¥...", expanded=True) as status:
        status.write("ğŸ” å®šä½ç›®æ ‡è´¦å·...")
        accounts = crawler.search_account(target_query)
        if not accounts:
            status.update(label="æœªæ‰¾åˆ°è´¦å·ï¼Œå¯èƒ½æ˜¯Cookieå·²å¤±æ•ˆ", state="error")
            st.stop()
        
        target = accounts[0]
        status.write(f"âœ… é”å®š: {target['nickname']}")
        
        status.write("ğŸ“ƒ æ‹‰å–æ–‡ç« åˆ—è¡¨...")
        raw_list = crawler.fetch_article_list(target['fakeid'], pages=scrape_pages)
        
        if not raw_list:
             status.update(label="æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥å‡­è¯", state="error")
             st.stop()

        status.write("ğŸ§¹ æ·±åº¦é‡‡é›†æ­£æ–‡å†…å®¹...")
        df_res = process_data(raw_list, crawler, fetch_details=enable_details)
        
        status.update(label="æ•°æ®å‡†å¤‡å°±ç»ª!", state="complete")
        st.session_state['data'] = df_res
        st.session_state['account'] = target['nickname']

if 'data' in st.session_state:
    df = st.session_state['data']
    nickname = st.session_state['account']
    st.header(f"ğŸ“° {nickname} Â· æ·±åº¦é˜…è¯»çœ‹æ¿")
    
    tab_read, tab_list = st.tabs(["ğŸ‘“ é˜…è¯»æ¨¡å¼", "ğŸ“‹ æ–‡ç« åˆ—è¡¨"])
    
    with tab_read:
        if 'content' in df.columns and not df['content'].isna().all():
            df['select_label'] = df['date'].astype(str) + " | " + df['title']
            selected_article_label = st.selectbox("é€‰æ‹©æ–‡ç« :", df['select_label'].tolist())
            article = df[df['select_label'] == selected_article_label].iloc[0]
            
            with st.container():
                st.markdown(f"## {article['title']}")
                st.caption(f"ä½œè€…: {article['author']} | å‘å¸ƒæ—¶é—´: {article['publish_time']} | {article['is_original']}")
                st.divider()
                if article['content']:
                    st.markdown(article['content'].replace("\n", "\n\n"))
                else:
                    st.warning("æ­£æ–‡å†…å®¹ä¸ºç©º")
                    st.markdown(f"[ç‚¹å‡»è·³è½¬åŸæ–‡é“¾æ¥]({article['link']})")
        else:
            st.info("æš‚æ— æ­£æ–‡æ•°æ®ï¼Œè¯·ç¡®ä¿å‹¾é€‰äº†ã€é‡‡é›†æ­£æ–‡ã€‘å¹¶é‡æ–°æŠ“å–ã€‚")
            
    with tab_list:
        st.dataframe(
            df[['title', 'date', 'author', 'is_original', 'link']],
            use_container_width=True,
            column_config={"link": st.column_config.LinkColumn("åŸæ–‡é“¾æ¥")}
        )
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é€‰æ‹©æµè§ˆå™¨å¹¶ç‚¹å‡» **'ä¸€é”®å”¤èµ·æ‰«ç '**ã€‚")

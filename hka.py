import streamlit as st
import pandas as pd
import requests
import time
import random
import os
import sys
import subprocess
import jieba
import matplotlib.pyplot as plt
import collections
import re
import json
from wordcloud import WordCloud
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from playwright.sync_api import sync_playwright
from io import BytesIO
import base64

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="å…¬ä¼—å·èˆ†æƒ…åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# å­—ä½“ç®¡ç† (è§£å†³ä¸­æ–‡ä¹±ç )
# ==========================================

def get_font_path():
    """
    è·å–ä¸­æ–‡å­—ä½“è·¯å¾„ã€‚å¦‚æœç³»ç»Ÿæ²¡æœ‰ï¼Œå°è¯•ä¸‹è½½ SimHeiã€‚
    """
    local_font = "SimHei.ttf"
    if os.path.exists(local_font):
        return local_font
    
    system_fonts = [
        "/System/Library/Fonts/PingFang.ttc", # MacOS
        "/System/Library/Fonts/STHeiti Light.ttc",
        "C:\\Windows\\Fonts\\simhei.ttf", # Windows
        "C:\\Windows\\Fonts\\msyh.ttc", 
        "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc", # Linux
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
    ]
    for path in system_fonts:
        if os.path.exists(path):
            return path
            
    try:
        url = "https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf"
        res = requests.get(url, timeout=30)
        with open(local_font, "wb") as f:
            f.write(res.content)
        return local_font
    except:
        pass
        
    return None

font_path = get_font_path()
if font_path:
    import matplotlib.font_manager as fm
    fe = fm.FontEntry(fname=font_path, name='CustomFont')
    fm.fontManager.ttflist.insert(0, fe)
    plt.rcParams['font.sans-serif'] = ['CustomFont']
else:
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# æ ¸å¿ƒå·¥å…·ç±»
# ==========================================

def clean_wechat_html(html_content):
    if not html_content:
        return "<div style='padding:20px; text-align:center; color:#999'>ğŸ“­ æ­£æ–‡å†…å®¹ä¸ºç©º</div>"
    
    soup = BeautifulSoup(html_content, "html.parser")
    
    content_div = soup.find("div", id="js_content")
    if content_div:
        existing_style = content_div.get('style', '')
        content_div['style'] = existing_style + '; visibility: visible !important; opacity: 1 !important;'
    
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()

    for img in soup.find_all("img"):
        if "data-src" in img.attrs:
            img["src"] = img["data-src"]
        img["referrerpolicy"] = "no-referrer"
        img["style"] = "max-width: 100% !important; height: auto !important; display: block; margin: 10px auto; border-radius: 4px;"

    wrapper = f"""
    <div style="
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        line-height: 1.8;
        color: #333;
        font-size: 16px;
        background-color: #fff;
        padding: 15px;
    ">
        {str(soup)}
    </div>
    """
    return wrapper

def get_name_tokens(name_list):
    """
    å¯¹å…¬ä¼—å·åç§°åˆ—è¡¨è¿›è¡Œåˆ†è¯ï¼Œç”Ÿæˆéœ€è¦å±è”½çš„è¯é›†åˆ
    """
    stop_tokens = set()
    for name in name_list:
        # 1. æ·»åŠ å…¨å
        stop_tokens.add(name)
        # 2. ç»“å·´åˆ†è¯ (ä¾‹å¦‚ 'é‡åº†å¾·æ™®å¤–å›½è¯­å­¦æ ¡' -> 'é‡åº†', 'å¾·æ™®', 'å¤–å›½è¯­', 'å­¦æ ¡')
        tokens = jieba.lcut(name)
        for t in tokens:
            if len(t) > 1: # åªæ·»åŠ ä¸¤ä¸ªå­—ä»¥ä¸Šçš„è¯
                stop_tokens.add(t)
        # 3. å°è¯•ç®€å•çš„ç®€ç§°è§„åˆ™ (å–å‰ä¸¤ä¸ªå­—ï¼Œåä¸¤ä¸ªå­—)
        if len(name) >= 4:
            stop_tokens.add(name[:2]) # ä¾‹å¦‚ 'é‡åº†'
            stop_tokens.add(name[-2:]) # ä¾‹å¦‚ 'å­¦æ ¡'
    return stop_tokens

def generate_wordcloud_img(text_data, exclude_words=None):
    """
    ç”Ÿæˆè¯äº‘å›¾ç‰‡å¯¹è±¡
    exclude_words: éœ€è¦é¢å¤–å±è”½çš„è¯åˆ—è¡¨ (list or set)
    """
    if not text_data:
        return None, []
        
    f_path = get_font_path()
    words = jieba.lcut(text_data)
    
    # åŸºç¡€åœç”¨è¯è¡¨ (ç²¾ç®€ç‰ˆ)
    stop_words = set([
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'åœ¨', 'ä¸º', 'å¯¹', 'ç­‰', 'ç¯‡', 
        'å¾®', 'ä¿¡', 'å·', 'æœˆ', 'æ—¥', 'å¹´', 'æœ‰', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£',
        'æˆ‘ä»¬', 'å›¾ç‰‡', 'æ¥æº', 'åŸæ ‡', 'é¢˜', 'å…¬ä¼—', 'ç‚¹å‡»', 'é˜…è¯»', 'åŸæ–‡', 'ä¸‹æ–¹', 'å…³æ³¨',
        'å±•å¼€', 'å…¨æ–‡', 'è§†é¢‘', 'åˆ†äº«', 'æ”¶è—', 'ç‚¹èµ', 'åœ¨çœ‹', 'æ‰«ç ', 'è¯†åˆ«', 'äºŒç»´ç ',
        'å®˜æ–¹', 'å¹³å°', 'å‘å¸ƒ', 'èµ„è®¯', 'æœåŠ¡', 'æŸ¥çœ‹', 'æ›´å¤š', 'å›å¤', 'å…³é”®å­—',
        'å­¦æ ¡', 'æ•™è‚²', 'å­¦é™¢', 'å¤§å­¦', 'ä¸­å­¦', 'å°å­¦', 'å¹¼å„¿å›­', 'å›½é™…', 'å¤–å›½è¯­', 'æ ¡åŒº' # è¡Œä¸šé€šç”¨è¯ä¿ç•™å±è”½
        # å·²ç§»é™¤ï¼š'è€å¸ˆ', 'å­¦ç”Ÿ', 'å®¶é•¿', 'åŒå­¦', 'å­©å­', 'è¯¾ç¨‹', 'æ´»åŠ¨'ï¼Œè®©è¿™äº›è¯å¯ä»¥æ˜¾ç¤º
    ])
    
    # åˆå¹¶ä¼ å…¥çš„è‡ªå®šä¹‰å±è”½è¯
    if exclude_words:
        stop_words.update(exclude_words)
    
    filtered_words = [w for w in words if len(w) > 1 and w not in stop_words]
    space_split_text = " ".join(filtered_words)
    
    if not space_split_text.strip():
        return None, []

    try:
        wc = WordCloud(
            font_path=f_path,
            width=800,
            height=400,
            background_color='white',
            max_words=150,
            colormap='viridis',
            prefer_horizontal=0.9
        ).generate(space_split_text)
        return wc, filtered_words
    except Exception as e:
        print(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
        return None, []

# ==========================================
# AI åˆ†ææ¨¡å— (SiliconFlow)
# ==========================================

def call_ai_analysis(data_df):
    """
    è°ƒç”¨ SiliconFlow API è¿›è¡Œ AI åˆ†æ
    """
    # 1. å‡†å¤‡ Key (ä¼˜å…ˆä» Secret è·å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤)
    api_key = st.secrets.get("SILICONFLOW_API_KEY", "sk-lezqyzzxlcnarawzhmyddltuclijckeufnzzktmkizfslcje")
    
    # 2. æ•°æ®æ±‡æ€»ä¸ç²¾ç®€ (ä¸å†å‘é€å…¨æ–‡ï¼Œè€Œæ˜¯æå–æ‘˜è¦ç»Ÿè®¡)
    summary_data = []
    
    # è·å–éœ€è¦å±è”½çš„è´¦å·åè¯ï¼Œç”¨äºæå–çƒ­è¯æ—¶æ’é™¤
    all_accounts = data_df['account_name'].unique()
    stop_tokens = get_name_tokens(all_accounts) 
    # æ·»åŠ ä¸€äº›åŸºç¡€åœç”¨è¯ç”¨äºæå–çƒ­è¯
    base_stops = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'æœ‰', 'ç­‰', 'ä¸º', 'ä¹‹', 'ä¸', 'åŠ', 'ä»¥', 'å¾®', 'ä¿¡', 'å…¬ä¼—', 'å·', 'æ‰«ç ', 'äºŒç»´ç ', 'å…³æ³¨', 'é˜…è¯»', 'åŸæ–‡', 'ç‚¹å‡»', 'æŸ¥çœ‹', 'æ›´å¤š', 'æ¥æº', 'å›¾ç‰‡', 'è§†é¢‘', 'å±•å¼€', 'å…¨æ–‡'}
    stop_tokens.update(base_stops)

    for account_name in all_accounts:
        sub_df = data_df[data_df['account_name'] == account_name]
        
        # æå–é«˜é¢‘è¯ (åŸºäºæ ‡é¢˜å’Œæ‘˜è¦ï¼Œæ¯”è¾ƒèŠ‚çœèµ„æºä¸”èƒ½åæ˜ ä¸»é¢˜)
        text_content = " ".join(sub_df['title'].tolist() + sub_df['digest'].fillna("").tolist())
        words = jieba.lcut(text_content)
        words = [w for w in words if len(w) > 1 and w not in stop_tokens]
        top_keywords = [w for w, c in collections.Counter(words).most_common(8)] # å–å‰8ä¸ªçƒ­è¯
        
        summary_data.append({
            "å…¬ä¼—å·å": account_name,
            "ç»Ÿè®¡å‘¨æœŸå†…å‘æ–‡æ•°": len(sub_df),
            "æœ€æ–°å‘å¸ƒæ—¥æœŸ": str(sub_df['å‘å¸ƒæ—¶é—´'].max()),
            "æ–‡ç« æ ‡é¢˜åˆ—è¡¨": sub_df['title'].tolist(),
            "æ ¸å¿ƒè®®é¢˜(çƒ­è¯)": top_keywords
        })
    
    data_json = json.dumps(summary_data, ensure_ascii=False, indent=2)
    
    # 3. æ„é€ è¯·æ±‚
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # ä¼˜åŒ–åçš„ Prompt
    system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•™è‚²è¡Œä¸šæ–°åª’ä½“æ•°æ®åˆ†æä¸“å®¶ã€‚
ç”¨æˆ·å°†æä¾›ä¸€ä»½JSONæ ¼å¼çš„æ±‡æ€»æ•°æ®ï¼ŒåŒ…å«å¤šä¸ªå…¬ä¼—å·åœ¨è¿‘æœŸçš„å‘æ–‡ç»Ÿè®¡ã€æ ‡é¢˜åˆ—è¡¨åŠæå–çš„é«˜é¢‘çƒ­è¯ã€‚
è¯·åŸºäºè¿™äº›ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½ã€æ·±åº¦èˆ†æƒ…å¯¹æ¯”åˆ†ææŠ¥å‘Šã€‘ã€‚

æŠ¥å‘Šåº”åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»´åº¦ï¼š
1. **æ ¸å¿ƒè®®é¢˜æ¦‚è§ˆ**ï¼šç»“åˆæ ‡é¢˜å’Œçƒ­è¯ï¼Œåˆ†æå„å…¬ä¼—å·è¿‘æœŸå…³æ³¨çš„é‡ç‚¹è¯é¢˜ï¼ˆå¦‚ï¼šæ‹›ç”Ÿå®£ä¼ ã€æ ¡å›­æ´»åŠ¨ã€å­¦æœ¯è®²åº§ã€èŠ‚æ—¥çƒ­ç‚¹ç­‰ï¼‰ã€‚
2. **æ´»è·ƒåº¦ä¸ç­–ç•¥å¯¹æ¯”**ï¼šå¯¹æ¯”å„è´¦å·çš„å‘æ–‡é¢‘ç‡ï¼Œåˆ†æå…¶è¿è¥æ´»è·ƒåº¦å·®å¼‚ã€‚
3. **å†…å®¹é£æ ¼æ´å¯Ÿ**ï¼šé€šè¿‡æ ‡é¢˜å…³é”®è¯åˆ†æå„è´¦å·çš„è¡Œæ–‡é£æ ¼ï¼ˆä¾‹å¦‚ï¼šæ ‡é¢˜å…šã€å­¦æœ¯ä¸¥è°¨ã€äº²æ°‘æ´»æ³¼ç­‰ï¼‰ã€‚
4. **æ€»ç»“ä¸å»ºè®®**ï¼šä¸ºè¿è¥è€…æä¾›åŸºäºæ•°æ®çš„ä¼˜åŒ–å»ºè®®ã€‚

è¯·åŠ¡å¿…åœ¨å›ç­”å¼€å¤´å±•ç¤ºä½ çš„ã€æ€è€ƒè¿‡ç¨‹ã€‘(Reasoning)ï¼Œç„¶åå†è¾“å‡ºç»“æ„æ¸…æ™°çš„æœ€ç»ˆæŠ¥å‘Šã€‚"""
    
    payload = {
        "model": "moonshotai/Kimi-K2-Thinking", 
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¿™æ˜¯å„å…¬ä¼—å·çš„æ•°æ®æ±‡æ€»ï¼Œè¯·è¿›è¡Œåˆ†æï¼š\n{data_json}"}
        ],
        "stream": False,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=120)
        if response.status_code == 200:
            res_json = response.json()
            choice = res_json.get('choices', [{}])[0]
            message = choice.get('message', {})
            content = message.get('content', '')
            reasoning = message.get('reasoning_content', '') 
            return True, content, reasoning
        else:
            return False, f"API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}", ""
    except Exception as e:
        return False, f"å‘ç”Ÿé”™è¯¯: {str(e)}", ""

# ==========================================
# çˆ¬è™«ç±»
# ==========================================

class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def check_auth(self):
        url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": "test", "begin": "0", "count": "1"
        }
        try:
            res = self.session.get(url, params=params)
            data = res.json()
            if "base_resp" in data and data["base_resp"]["ret"] != 0:
                return False, f"Token å¤±æ•ˆæˆ– Cookie è¿‡æœŸ: {data['base_resp']}"
            return True, "éªŒè¯é€šè¿‡"
        except:
            return False, "ç½‘ç»œè¿æ¥å¼‚å¸¸"

    def search_account(self, query):
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params, timeout=10)
            data = res.json()
            return data.get("list", [])
        except:
            return []

    def fetch_article_list(self, fakeid, pages=1):
        all_articles = []
        for page in range(pages):
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params, timeout=10)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                time.sleep(random.uniform(0.5, 1.5))
            except:
                break
        return all_articles

    def fetch_content(self, url):
        try:
            res = self.session.get(url, timeout=15)
            soup = BeautifulSoup(res.text, "html.parser")
            content_div = soup.find("div", id="js_content")
            
            if content_div:
                final_html = clean_wechat_html(str(soup))
                plain_text = content_div.get_text(strip=True)
            else:
                final_html = clean_wechat_html(res.text)
                plain_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"}) or soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return final_html, author, plain_text
        except Exception:
            return "<div>è·å–å¤±è´¥</div>", "è·å–å¤±è´¥", ""

# ==========================================
# è‡ªåŠ¨åŒ–ç™»å½•
# ==========================================

def force_install_chromium():
    try:
        cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except:
        return False

def auto_login_browser():
    status_box = st.empty()
    qr_box = st.empty()
    token = None
    cookie_str = None

    status_box.info("ğŸš€ æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–å¼•æ“ (é«˜æ¸…äº‘ç«¯æ¨¡å¼)...")

    try:
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
            except Exception as e:
                if "Executable doesn't exist" in str(e):
                    status_box.warning("âš™ï¸ æ­£åœ¨å®‰è£…æµè§ˆå™¨å†…æ ¸...")
                    if force_install_chromium():
                         status_box.success("âœ… å®‰è£…æˆåŠŸï¼é‡è¯•ä¸­...")
                         browser = p.chromium.launch(headless=True)
                    else:
                         return None, None
                else:
                    raise e

            context = browser.new_context(viewport={'width': 1920, 'height': 1080})
            page = context.new_page()

            status_box.info("ğŸ”— æ­£åœ¨åŠ è½½å¾®ä¿¡ç™»å½•é¡µ...")
            page.goto("https://mp.weixin.qq.com/")
            
            try:
                page.wait_for_selector(".login__type__container__scan", timeout=15000)
            except:
                pass 

            status_box.warning("ğŸ“± è¯·ä½¿ç”¨æ‰‹æœºå¾®ä¿¡æ‰«ç  (äºŒç»´ç å·²æ”¾å¤§):")
            
            max_wait = 120
            for i in range(max_wait):
                try:
                    if page.is_closed(): return None, None
                    current_url = page.url
                except: return None, None

                if i % 1.5 == 0 and "token=" not in current_url:
                    try:
                        qr_elem = page.locator(".login__type__container__scan")
                        if qr_elem.count() > 0:
                            screenshot_bytes = qr_elem.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å®æ—¶ç”»é¢)", width=400)
                        else:
                            screenshot_bytes = page.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å…¨å±å¤‡ç”¨)", width=600)
                    except:
                        pass

                if "token=" in current_url:
                    qr_box.empty()
                    status_box.success(f"âœ… ç™»å½•æˆåŠŸï¼")
                    
                    parsed = urlparse(current_url)
                    token = parse_qs(parsed.query).get("token", [""])[0]
                    cookies = context.cookies()
                    cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                    time.sleep(1)
                    break
                else:
                    time.sleep(1)
            
            browser.close()
            
    except Exception as e:
        status_box.error(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        return None, None

    return token, cookie_str

# ==========================================
# Streamlit ä¸»ç•Œé¢
# ==========================================

if 'wx_token' not in st.session_state: st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state: st.session_state['wx_cookie'] = ''
if 'all_data' not in st.session_state: st.session_state['all_data'] = None

with st.sidebar:
    st.title("ğŸ“ å…¬ä¼—å·èˆ†æƒ…åˆ†æ Pro")
    st.caption("Playwright Â· è¯äº‘ Â· AI æ·±åº¦æ€è€ƒ")
    st.markdown("---")
    
    if st.button("ğŸ“¢ 1. æ‰«ç è·å–æƒé™", type="primary", use_container_width=True):
        token, cookie = auto_login_browser()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.success("æƒé™è·å–æˆåŠŸï¼")
            time.sleep(1)
            st.rerun()

    with st.expander("ğŸ”‘ å‡­è¯ç®¡ç†", expanded=True):
        token_input = st.text_input("Token", value=st.session_state['wx_token'])
        cookie_input = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=100)
        
        if token_input != st.session_state['wx_token']: st.session_state['wx_token'] = token_input
        if cookie_input != st.session_state['wx_cookie']: st.session_state['wx_cookie'] = cookie_input

    st.markdown("---")
    targets_input = st.text_area(
        "2. è¾“å…¥å…¬ä¼—å·åç§°", 
        placeholder="æ”¯æŒä¸­æ–‡é€—å·ã€é¡¿å·ã€ç©ºæ ¼æˆ–æ¢è¡Œåˆ†éš”\nä¾‹å¦‚ï¼š\næ¸…åå¤§å­¦ã€åŒ—äº¬å¤§å­¦ï¼Œå¤æ—¦å¤§å­¦",
        height=150
    )
    
    page_count = st.slider("æ¯ä¸ªå·æŠ“å–é¡µæ•° (æ¯é¡µ5ç¯‡)", 1, 5, 2)
    run_btn = st.button("ğŸš€ 3. å¼€å§‹åˆ†æ", use_container_width=True)

# --- ä¸»é€»è¾‘åŒº ---

if run_btn:
    if not token_input or not cookie_input:
        st.error("è¯·å…ˆè·å– Token å’Œ Cookieï¼")
        st.stop()
    if not targets_input.strip():
        st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·ï¼")
        st.stop()
        
    target_list = re.split(r'[,\s\nï¼Œã€]+', targets_input.strip())
    target_list = [t for t in target_list if t]
    
    crawler = WechatCrawler(token_input, cookie_input)
    
    all_results = []
    status_container = st.status("æ­£åœ¨è¿›è¡Œæ•°æ®é‡‡é›†...", expanded=True)
    progress_bar = st.progress(0)
    
    with status_container:
        if not crawler.check_auth()[0]:
            st.error("æƒé™éªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°æ‰«ç ï¼")
            st.stop()
            
        total_targets = len(target_list)
        for i, target_name in enumerate(target_list):
            st.write(f"ğŸ”„ [{i+1}/{total_targets}] åˆ†æ: **{target_name}** ...")
            
            accounts = crawler.search_account(target_name)
            if not accounts:
                st.warning(f"âš ï¸ æœªæ‰¾åˆ°: {target_name}ï¼Œè·³è¿‡")
                continue
            
            target_account = accounts[0]
            fakeid = target_account['fakeid']
            real_nickname = target_account['nickname']
            
            articles = crawler.fetch_article_list(fakeid, pages=page_count)
            
            if articles:
                st.write(f"   - æŠ“å–æ­£æ–‡ ({len(articles)}ç¯‡)...")
                for art in articles:
                    html_content, author, plain_text = crawler.fetch_content(art['link'])
                    art['content_html'] = html_content
                    art['plain_text'] = plain_text
                    art['author'] = author
                    art['account_name'] = real_nickname
                    time.sleep(0.5)

            all_results.extend(articles)
            progress_bar.progress((i + 1) / total_targets)
            time.sleep(random.uniform(1.0, 2.0))
            
        status_container.update(label="âœ… é‡‡é›†ä¸åˆ†æå®Œæˆï¼", state="complete")
    
    if all_results:
        df = pd.DataFrame(all_results)
        df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['create_time'], unit='s')
        df['å‘å¸ƒæ—¥æœŸ'] = df['å‘å¸ƒæ—¶é—´'].dt.date
        df['ç±»å‹'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
        st.session_state['all_data'] = df
    else:
        st.warning("æœªé‡‡é›†åˆ°æ•°æ®ã€‚")

# --- åˆ†æçœ‹æ¿ ---

if st.session_state['all_data'] is not None:
    df = st.session_state['all_data']
    
    st.divider()
    st.title("ğŸ“Š å…¬ä¼—å·æ–°åª’ä½“å¤§æ•°æ®çœ‹æ¿")
    
    # 4ä¸ª Tabï¼Œæœ€åä¸€ä¸ªæ˜¯ AI åˆ†æ
    tab_global_1, tab_global_2, tab_global_3, tab_ai = st.tabs(["â˜ï¸ ç»¼åˆè¯äº‘", "ğŸ† å½±å“åŠ›æ’è¡Œ", "ğŸ“ˆ å‘æ–‡è¶‹åŠ¿", "ğŸ¤– AI æ·±åº¦æŠ¥å‘Š"])
    
    # è·å–æ‰€æœ‰å…¬ä¼—å·åç§°å¹¶ç”Ÿæˆå±è”½è¯
    all_accounts = df['account_name'].unique()
    global_stop_words = get_name_tokens(all_accounts)
    
    with tab_global_1:
        col_g1, col_g2 = st.columns(2)
        with col_g1:
            st.subheader("å…¨ç½‘Â·æ ‡é¢˜è¯äº‘")
            all_titles = " ".join(df['title'].tolist())
            wc_title, _ = generate_wordcloud_img(all_titles, exclude_words=global_stop_words)
            if wc_title:
                st.image(wc_title.to_array(), use_container_width=True)
            else:
                st.info("æ•°æ®ä¸è¶³")
                
        with col_g2:
            st.subheader("å…¨ç½‘Â·å†…å®¹è¯äº‘")
            all_contents = " ".join(df['plain_text'].fillna("").tolist())
            wc_content, words_list = generate_wordcloud_img(all_contents, exclude_words=global_stop_words)
            if wc_content:
                st.image(wc_content.to_array(), use_container_width=True)
            
            if words_list:
                st.markdown("**ğŸ”¥ å…¨ç½‘ TOP 10 çƒ­è¯:**")
                counts = collections.Counter(words_list)
                top10 = counts.most_common(10)
                top10_df = pd.DataFrame(top10, columns=['å…³é”®è¯', 'é¢‘æ¬¡'])
                st.dataframe(top10_df.T, use_container_width=True)

    with tab_global_2:
        st.caption("æ³¨ï¼šæ•°æ®åŸºäºæœ¬æ¬¡æŠ“å–çš„æ ·æœ¬è®¡ç®—ã€‚")
        col_r1, col_r2 = st.columns(2)
        now = pd.Timestamp.now()
        
        with col_r1:
            st.markdown("#### ğŸ“… æœ¬å‘¨å‘æ–‡æ¦œ")
            week_df = df[df['å‘å¸ƒæ—¶é—´'] > (now - pd.Timedelta(days=7))]
            if not week_df.empty:
                week_rank = week_df['account_name'].value_counts().reset_index()
                week_rank.columns = ['å…¬ä¼—å·', 'å‘æ–‡æ•°']
                st.dataframe(week_rank, use_container_width=True, hide_index=True)
            else:
                st.info("æœ¬å‘¨æ— æ•°æ®")
                
        with col_r2:
            st.markdown("#### ğŸ—“ï¸ æœ¬æœˆå‘æ–‡æ¦œ")
            month_df = df[df['å‘å¸ƒæ—¶é—´'] > (now - pd.Timedelta(days=30))]
            if not month_df.empty:
                month_rank = month_df['account_name'].value_counts().reset_index()
                month_rank.columns = ['å…¬ä¼—å·', 'å‘æ–‡æ•°']
                st.dataframe(month_rank, use_container_width=True, hide_index=True)
            else:
                st.info("æœ¬æœˆæ— æ•°æ®")

    with tab_global_3:
        st.subheader("å…¨ç½‘å‘å¸ƒæ—¶é—´åˆ†å¸ƒ")
        date_counts = df.groupby('å‘å¸ƒæ—¥æœŸ').size()
        st.line_chart(date_counts)

    # --- AI åˆ†æ Tab ---
    with tab_ai:
        st.subheader("ğŸ¤– Kimi-K2-Thinking æ·±åº¦èˆ†æƒ…æŠ¥å‘Š")
        st.info("å°†ä½¿ç”¨ SiliconFlow API å¯¹é‡‡é›†åˆ°çš„æ‰€æœ‰æ•°æ®è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æã€‚")
        
        if st.button("ğŸ§  å¼€å§‹ AI æ·±åº¦æ€è€ƒä¸åˆ†æ", type="primary"):
            with st.spinner("AI æ­£åœ¨é˜…è¯»æ•°æ®å¹¶è¿›è¡Œæ¨ç† (è¿™å¯èƒ½éœ€è¦ 30-60 ç§’)..."):
                success, report, reasoning = call_ai_analysis(df)
            
            if success:
                # 1. å±•ç¤ºæ€è€ƒè¿‡ç¨‹
                if reasoning:
                    with st.expander("ğŸ’­ ç‚¹å‡»æŸ¥çœ‹ AI çš„æ€è€ƒè¿‡ç¨‹ (Reasoning)", expanded=True):
                        st.markdown(reasoning)
                    st.divider()
                
                # 2. å±•ç¤ºæœ€ç»ˆæŠ¥å‘Š
                st.markdown(report)
                st.success("åˆ†æå®Œæˆï¼")
            else:
                st.error(report)

    st.markdown("---")

    # 2. ä¸ªä½“ç”»åƒ
    st.header("2. å•å·æ·±åº¦ç”»åƒ (Single Account)")
    
    account_list = df['account_name'].unique()
    selected_account = st.selectbox("ğŸ‘‰ é€‰æ‹©ä¸€ä¸ªå…¬ä¼—å·æŸ¥çœ‹è¯¦æƒ…:", account_list)
    
    if selected_account:
        sub_df = df[df['account_name'] == selected_account].copy()
        
        # é’ˆå¯¹å•ä¸ªè´¦å·ç”Ÿæˆç‰¹å®šçš„å±è”½è¯
        account_stop_words = get_name_tokens([selected_account])
        
        c1, c2, c3 = st.columns(3)
        c1.metric("æ€»å‘æ–‡æ•°", len(sub_df))
        c2.metric("åŸåˆ›æ¯”ä¾‹", f"{len(sub_df[sub_df['ç±»å‹']=='åŸåˆ›']) / len(sub_df) * 100:.1f}%" if len(sub_df)>0 else "0%")
        c3.metric("æœ€æ–°å‘å¸ƒ", str(sub_df['å‘å¸ƒæ—¥æœŸ'].max()))
        
        tab_s1, tab_s2 = st.tabs(["ğŸ“Š æ•°æ®åˆ†æ", "ğŸ“° æ–‡ç« åˆ—è¡¨ä¸é˜…è¯»"])
        
        with tab_s1:
            sc1, sc2 = st.columns(2)
            with sc1:
                st.markdown("**æ ‡é¢˜è¯äº‘**")
                s_titles = " ".join(sub_df['title'].tolist())
                s_wc_t, _ = generate_wordcloud_img(s_titles, exclude_words=account_stop_words)
                if s_wc_t: st.image(s_wc_t.to_array(), use_container_width=True)
                
            with sc2:
                st.markdown("**å†…å®¹è¯äº‘**")
                s_content = " ".join(sub_df['plain_text'].fillna("").tolist())
                s_wc_c, s_words = generate_wordcloud_img(s_content, exclude_words=account_stop_words)
                if s_wc_c: 
                    st.image(s_wc_c.to_array(), use_container_width=True)
                    if s_words:
                        st.markdown("**ğŸ”¥ TOP 10 çƒ­è¯:**")
                        s_counts = collections.Counter(s_words)
                        st.json(dict(s_counts.most_common(10)))

        with tab_s2:
            col_list, col_read = st.columns([1, 2])
            
            with col_list:
                st.markdown("##### æ–‡ç« åˆ—è¡¨")
                sub_df['label'] = sub_df.apply(lambda x: f"{x['å‘å¸ƒæ—¥æœŸ']} | {x['title']}", axis=1)
                
                selected_article_label = st.radio(
                    "ç‚¹å‡»é€‰æ‹©æ–‡ç« é˜…è¯»:",
                    sub_df['label'].tolist(),
                    label_visibility="collapsed"
                )
            
            with col_read:
                if selected_article_label:
                    read_art = sub_df[sub_df['label'] == selected_article_label].iloc[0]
                    
                    st.markdown(f"#### {read_art['title']}")
                    st.caption(f"âœï¸ {read_art['author']}  |  ğŸ•’ {read_art['å‘å¸ƒæ—¶é—´']}")
                    st.divider()
                    
                    if 'content_html' in read_art and read_art['content_html']:
                        st.components.v1.html(read_art['content_html'], height=800, scrolling=True)
                    else:
                        st.warning("æ­£æ–‡å†…å®¹ä¸ºç©º")

else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ æ“ä½œï¼šæ‰«ç  -> è¾“å…¥å…¬ä¼—å· -> å¼€å§‹åˆ†æ")

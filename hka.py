import streamlit as st
import pandas as pd
import requests
import time
import random
import os
import sys
import subprocess
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from playwright.sync_api import sync_playwright

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="å…¬ä¼—å·æ‰¹é‡é‡‡é›† & åˆ†æç¥å™¨",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# æ ¸å¿ƒå·¥å…·ç±»ï¼šè´Ÿè´£æå®šæ•°æ®å’ŒHTMLæ¸…æ´—
# ==========================================

def clean_wechat_html(html_content):
    """
    æ¸…æ´—å¾®ä¿¡HTMLï¼Œç ´è§£å›¾ç‰‡é˜²ç›—é“¾ï¼Œé€‚é…ç½‘é¡µæ˜¾ç¤º
    """
    if not html_content:
        return "<div style='padding:20px; text-align:center; color:#999'>ğŸ“­ æ­£æ–‡å†…å®¹ä¸ºç©º</div>"
    
    soup = BeautifulSoup(html_content, "html.parser")
    
    # 1. ç ´è§£å›¾ç‰‡é˜²ç›—é“¾ & ä¿®å¤æ‡’åŠ è½½
    for img in soup.find_all("img"):
        if "data-src" in img.attrs:
            img["src"] = img["data-src"]
        
        # å¼ºåˆ¶æ ·å¼ï¼šè‡ªé€‚åº”å®½åº¦
        img["style"] = "max-width: 100% !important; height: auto !important; display: block; margin: 10px auto; border-radius: 4px;"
        img["referrerpolicy"] = "no-referrer"

    # 2. ç§»é™¤è§†é¢‘ iframe
    for iframe in soup.find_all("iframe"):
        iframe["style"] = "width: 100%; height: 300px; border: 1px solid #eee; background: #f9f9f9;"

    wrapper = f"""
    <div style="
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
        line-height: 1.8;
        color: #333;
        font-size: 16px;
        padding: 10px;
        background-color: #fff;
    ">
        {str(soup)}
    </div>
    """
    return wrapper

class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def check_auth(self):
        """ç®€å•éªŒè¯ Token æ˜¯å¦æœ‰æ•ˆ"""
        url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": "test", "begin": "0", "count": "1"
        }
        try:
            res = self.session.get(url, params=params)
            data = res.json()
            if "base_resp" in data and data["base_resp"]["ret"] != 0:
                return False, f"éªŒè¯å¤±è´¥: {data['base_resp']}"
            return True, "éªŒè¯é€šè¿‡"
        except:
            return False, "ç½‘ç»œè¿æ¥å¼‚å¸¸"

    def search_account(self, query):
        """æœç´¢å…¬ä¼—å·"""
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params, timeout=10)
            data = res.json()
            return data.get("list", [])
        except Exception as e:
            st.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def fetch_article_list(self, fakeid, pages=1):
        """è·å–æ–‡ç« åˆ—è¡¨"""
        all_articles = []
        
        # è¿™é‡Œçš„è¿›åº¦æ¡ç”±å¤–éƒ¨æ§åˆ¶ï¼Œè¿™é‡Œåªè´Ÿè´£æŠ“å–
        for page in range(pages):
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params, timeout=10)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break 
                time.sleep(random.uniform(1.5, 3.0)) # ç¨å¾®è°ƒå¤§å»¶æ—¶ï¼Œæ‰¹é‡æŠ“å–æ›´å®‰å…¨
            except:
                break
        return all_articles

    def fetch_content(self, url):
        """é‡‡é›†æ­£æ–‡ HTML"""
        try:
            res = self.session.get(url, timeout=15)
            soup = BeautifulSoup(res.text, "html.parser")
            content_div = soup.find("div", {"id": "js_content"})
            
            if content_div:
                final_html = clean_wechat_html(str(content_div))
            else:
                final_html = "<p>æ— æ³•è§£ææ­£æ–‡ç»“æ„</p>"
            
            author_tag = soup.find("strong", {"class": "profile_nickname"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return final_html, author
        except Exception:
            return "", "è·å–å¤±è´¥"

# ==========================================
# è‡ªåŠ¨åŒ–å·¥å…·ç±»
# ==========================================

def force_install_chromium():
    try:
        cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except Exception as e:
        return False

def auto_login_browser():
    status_box = st.empty()
    token = None
    cookie_str = None

    status_box.info("ğŸš€ æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–å¼•æ“ï¼Œè¯·ç¨å€™...")

    try:
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=False)
            except Exception as e:
                if "Executable doesn't exist" in str(e):
                    status_box.warning("âš™ï¸ é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…æµè§ˆå™¨å†…æ ¸...")
                    if force_install_chromium():
                         status_box.success("âœ… å®‰è£…æˆåŠŸï¼")
                         browser = p.chromium.launch(headless=False)
                    else:
                         return None, None
                else:
                    raise e

            context = browser.new_context()
            page = context.new_page()

            status_box.info("ğŸ”— æ­£åœ¨æ‰“å¼€å¾®ä¿¡å…¬ä¼—å¹³å°...")
            page.goto("https://mp.weixin.qq.com/")

            status_box.warning("ğŸ“± è¯·çœ‹æµè§ˆå™¨çª—å£ -> ç”¨å¾®ä¿¡æ‰«ç ç™»å½•")
            
            max_wait = 120
            for i in range(max_wait):
                try:
                    if page.is_closed(): return None, None
                    current_url = page.url
                except: return None, None

                if "token=" in current_url:
                    status_box.success(f"âœ… ç™»å½•æˆåŠŸï¼æ­£åœ¨æå–å¯†é’¥... ({i}s)")
                    parsed = urlparse(current_url)
                    token = parse_qs(parsed.query).get("token", [""])[0]
                    cookies = context.cookies()
                    cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                    time.sleep(1)
                    break
                else:
                    time.sleep(1)
            
            browser.close()
            
    except Exception as e:
        status_box.error(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return None, None

    return token, cookie_str

# ==========================================
# Streamlit ä¸»ç•Œé¢
# ==========================================

if 'wx_token' not in st.session_state: st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state: st.session_state['wx_cookie'] = ''
if 'all_data' not in st.session_state: st.session_state['all_data'] = None

with st.sidebar:
    st.title("ğŸ“Š æ‰¹é‡é‡‡é›†åˆ†æ")
    st.caption("æ”¯æŒå¤šè´¦å· Â· èšåˆåˆ†æ Â· å›¾æ–‡è¿˜åŸ")
    st.markdown("---")
    
    st.markdown("### 1. è·å–æƒé™")
    if st.button("ğŸ“¢ å”¤èµ·æµè§ˆå™¨æ‰«ç ", type="primary", use_container_width=True):
        token, cookie = auto_login_browser()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.balloons()
            st.success("ğŸ‰ è·å–æˆåŠŸï¼å‡­è¯å·²å¡«å…¥ã€‚")
            time.sleep(1)
            st.rerun()

    with st.expander("ğŸ”‘ å‡­è¯ç®¡ç†", expanded=True):
        token_input = st.text_input("Token", value=st.session_state['wx_token'])
        cookie_input = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=100)
        
        if token_input != st.session_state['wx_token']: st.session_state['wx_token'] = token_input
        if cookie_input != st.session_state['wx_cookie']: st.session_state['wx_cookie'] = cookie_input

    st.markdown("---")
    st.markdown("### 2. æ‰¹é‡è®¾ç½®")
    
    # æ‰¹é‡è¾“å…¥æ¡†
    targets_input = st.text_area(
        "è¾“å…¥å…¬ä¼—å·åç§° (ä¸€è¡Œä¸€ä¸ªï¼Œæœ€å¤š20ä¸ª)", 
        placeholder="36æ°ª\nè™å—…APP\næ™šç‚¹LatePost",
        height=150
    )
    
    page_count = st.slider("æ¯ä¸ªå·æŠ“å–é¡µæ•° (æ¯é¡µ5ç¯‡)", 1, 5, 2, help="æŠ“å–å¤ªå¤šé¡µå¯èƒ½ä¼šè§¦å‘å¾®ä¿¡é£æ§")
    need_detail = st.checkbox("æ·±åº¦é‡‡é›†æ­£æ–‡", value=True)
    
    run_btn = st.button("ğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›†", use_container_width=True)

# --- ä¸»é€»è¾‘åŒº ---

if run_btn:
    if not token_input or not cookie_input:
        st.error("âŒ è¯·å…ˆæ‰«ç è·å–æƒé™ï¼")
        st.stop()
    if not targets_input.strip():
        st.error("âŒ è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·åç§°ï¼")
        st.stop()
        
    target_list = [line.strip() for line in targets_input.split('\n') if line.strip()]
    if len(target_list) > 20:
        st.warning(f"âš ï¸ è¾“å…¥äº† {len(target_list)} ä¸ªè´¦å·ï¼Œè‡ªåŠ¨æˆªå–å‰ 20 ä¸ªã€‚")
        target_list = target_list[:20]
        
    crawler = WechatCrawler(token_input, cookie_input)
    
    all_results = []
    
    # å…¨å±€å®¹å™¨
    status_container = st.status("æ­£åœ¨åˆå§‹åŒ–é‡‡é›†ä»»åŠ¡...", expanded=True)
    progress_bar = st.progress(0)
    
    with status_container:
        # 1. éªŒè¯æƒé™
        st.write("ğŸ” éªŒè¯èº«ä»½æƒé™...")
        is_valid, msg = crawler.check_auth()
        if not is_valid:
            status_container.update(label="èº«ä»½éªŒè¯å¤±è´¥", state="error")
            st.error(msg)
            st.stop()
            
        st.write(f"ğŸ“‹ ä»»åŠ¡é˜Ÿåˆ—: å…± {len(target_list)} ä¸ªå…¬ä¼—å·")
        
        # 2. å¾ªç¯æŠ“å–
        for i, target_name in enumerate(target_list):
            st.write(f"ğŸ”„ [{i+1}/{len(target_list)}] æ­£åœ¨å¤„ç†: **{target_name}** ...")
            
            # æœç´¢è´¦å·
            accounts = crawler.search_account(target_name)
            if not accounts:
                st.warning(f"âš ï¸ æœªæ‰¾åˆ°å…¬ä¼—å·: {target_name}ï¼Œè·³è¿‡ã€‚")
                continue
                
            # é»˜è®¤å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
            target_account = accounts[0]
            fakeid = target_account['fakeid']
            real_nickname = target_account['nickname']
            
            # æŠ“å–åˆ—è¡¨
            articles = crawler.fetch_article_list(fakeid, pages=page_count)
            st.write(f"   - è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« æ‘˜è¦")
            
            # æ·±åº¦é‡‡é›†
            if need_detail and articles:
                st.write(f"   - æ­£åœ¨ä¸‹è½½æ­£æ–‡ ({len(articles)}ç¯‡)...")
                # ç®€å•çš„å†…éƒ¨è¿›åº¦
                for art in articles:
                    html_content, author = crawler.fetch_content(art['link'])
                    art['content_html'] = html_content
                    art['author'] = author
                    time.sleep(0.5) # é¿å…ç”±äºè¯·æ±‚è¿‡å¿«å¯¼è‡´IPè¢«å°
            
            # è¡¥å……å…ƒæ•°æ®
            for art in articles:
                art['account_name'] = real_nickname
                art['keyword'] = target_name
                
            all_results.extend(articles)
            
            # æ›´æ–°æ€»è¿›åº¦
            progress_bar.progress((i + 1) / len(target_list))
            
            # è´¦å·é—´å»¶æ—¶ï¼Œé˜²é£æ§
            time.sleep(random.uniform(2.0, 4.0))
            
        status_container.update(label="âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼", state="complete")
    
    # å¤„ç†æ•°æ®
    if all_results:
        df = pd.DataFrame(all_results)
        df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['create_time'], unit='s')
        df['å‘å¸ƒæ—¥æœŸ'] = df['å‘å¸ƒæ—¶é—´'].dt.date
        df['ç±»å‹'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
        st.session_state['all_data'] = df
    else:
        st.warning("æœªé‡‡é›†åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ã€‚")

# --- åˆ†æå±•ç¤ºåŒº ---

if st.session_state['all_data'] is not None:
    df = st.session_state['all_data']
    
    st.divider()
    st.title("ğŸ“ˆ å…¨ç½‘æ•°æ®åˆ†æçœ‹æ¿")
    
    # æ¦‚è§ˆæŒ‡æ ‡
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("å…¬ä¼—å·æ•°é‡", df['account_name'].nunique())
    c2.metric("æ–‡ç« æ€»æ•°", len(df))
    c3.metric("åŸåˆ›æ–‡ç« ", len(df[df['ç±»å‹']=='åŸåˆ›']))
    c4.metric("æœ€æ—©å‘å¸ƒ", str(df['å‘å¸ƒæ—¥æœŸ'].min()))
    
    tab_analysis, tab_data, tab_read = st.tabs(["ğŸ“Š å›¾è¡¨åˆ†æ", "ğŸ“‹ æ•°æ®æ˜ç»†", "ğŸ‘“ é˜…è¯»æ–‡ç« "])
    
    with tab_analysis:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("å„å…¬ä¼—å·å‘æ–‡é‡å¯¹æ¯”")
            count_data = df['account_name'].value_counts()
            st.bar_chart(count_data)
            
        with col2:
            st.subheader("åŸåˆ› vs è½¬è½½ æ¯”ä¾‹")
            type_counts = df['ç±»å‹'].value_counts()
            st.bar_chart(type_counts, horizontal=True, color="#ffaa00")
            
        st.subheader("å‘å¸ƒæ—¶é—´åˆ†å¸ƒ (æŒ‰æ—¥æœŸ)")
        time_chart = df.groupby('å‘å¸ƒæ—¥æœŸ').size()
        st.line_chart(time_chart)
        
    with tab_data:
        # æ•°æ®è¡¨
        display_cols = ['account_name', 'title', 'å‘å¸ƒæ—¶é—´', 'ç±»å‹', 'digest', 'link']
        if 'author' in df.columns: display_cols.insert(2, 'author')
        
        st.dataframe(
            df[display_cols],
            column_config={
                "link": st.column_config.LinkColumn("é“¾æ¥"),
                "account_name": "å…¬ä¼—å·",
                "title": "æ ‡é¢˜"
            },
            use_container_width=True
        )
        
        # ä¸‹è½½
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            "ğŸ“¥ ä¸‹è½½æ‰€æœ‰æ•°æ® (CSV)",
            data=csv,
            file_name='wechat_batch_data.csv',
            mime='text/csv',
            type="primary"
        )
        
    with tab_read:
        if 'content_html' in df.columns:
            # çº§è”é€‰æ‹©å™¨
            sel_account = st.selectbox("é€‰æ‹©å…¬ä¼—å·", df['account_name'].unique())
            sub_df = df[df['account_name'] == sel_account]
            
            sel_article_idx = st.selectbox(
                "é€‰æ‹©æ–‡ç« ", 
                sub_df.index, 
                format_func=lambda x: f"{sub_df.loc[x, 'å‘å¸ƒæ—¶é—´']} | {sub_df.loc[x, 'title']}"
            )
            
            if sel_article_idx is not None:
                article = df.loc[sel_article_idx]
                with st.container(border=True):
                    st.markdown(f"## {article['title']}")
                    st.caption(f"ğŸ“… {article['å‘å¸ƒæ—¶é—´']} | ğŸ‘¤ {article.get('author','')} | ğŸ·ï¸ {article['ç±»å‹']}")
                    st.divider()
                    st.components.v1.html(article['content_html'], height=600, scrolling=True)
        else:
            st.info("æœªé‡‡é›†æ­£æ–‡æ•°æ®")
else:
    st.info("ğŸ‘‹ è¯·åœ¨å·¦ä¾§è¾“å…¥å…¬ä¼—å·å¹¶å¼€å§‹é‡‡é›†ï¼Œæ•°æ®å°†åœ¨æ­¤å¤„å±•ç¤ºã€‚")

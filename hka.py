import streamlit as st
import pandas as pd
import requests
import time
import random
import datetime
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse, parse_qs
import shutil

# --- æ–°å¢ï¼šPlaywright åº“ ---
from playwright.sync_api import sync_playwright

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="WeChat Insight Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ ¸å¿ƒçˆ¬è™«é€»è¾‘ (è´Ÿè´£æŠ“å–æ•°æ®) ---
class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search_account(self, query):
        """æœç´¢å…¬ä¼—å·è·å–fakeid"""
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params)
            data = res.json()
            # æ£€æŸ¥æ˜¯å¦æœ‰æƒé™é”™è¯¯
            if "base_resp" in data and data["base_resp"]["ret"] != 0:
                st.error(f"å¾®ä¿¡æ¥å£æŠ¥é”™: {data['base_resp']}")
                return []
            return data.get("list", [])
        except Exception as e:
            st.error(f"æœç´¢è¯·æ±‚å¼‚å¸¸: {e}")
            return []

    def fetch_article_list(self, fakeid, pages=3):
        """è·å–æ–‡ç« åˆ—è¡¨å…ƒæ•°æ®"""
        all_articles = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for page in range(pages):
            status_text.text(f"ğŸ“¡ æ­£åœ¨æ‰«æåˆ—è¡¨ç¬¬ {page + 1}/{pages} é¡µ...")
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "item_idx": item.get("item_idx", 1),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                progress_bar.progress((page + 1) / pages)
                time.sleep(random.uniform(1.0, 2.0))
            except:
                break
        
        progress_bar.empty()
        status_text.empty()
        return all_articles

    def fetch_article_content(self, url):
        """æ·±åº¦é‡‡é›†ï¼šè®¿é—®è¯¦æƒ…é¡µè·å–æ­£æ–‡"""
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, "lxml")
            
            content_div = soup.find("div", {"id": "js_content"})
            if content_div:
                for p in content_div.find_all('p'):
                    p.insert_after('\n')
                content_text = content_div.get_text().strip()
            else:
                content_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"})
            if not author_tag:
                author_tag = soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return content_text, author, "IPæœªçŸ¥"
        except Exception:
            return "", "è·å–å¤±è´¥", "è·å–å¤±è´¥"

# --- æ•°æ®å¤„ç†å·¥å…· ---
def process_data(articles, crawler=None, fetch_details=False):
    if not articles:
        return pd.DataFrame()
    df = pd.DataFrame(articles)
    df['publish_time'] = pd.to_datetime(df['create_time'], unit='s')
    df['date'] = df['publish_time'].dt.date
    df['is_original'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
    
    if fetch_details and crawler:
        st.info("ğŸ¢ æ­£åœ¨æ·±åº¦é‡‡é›†å…¨æ–‡...")
        details = []
        bar = st.progress(0)
        for idx, row in df.iterrows():
            content, author, ip = crawler.fetch_article_content(row['link'])
            details.append({'content': content, 'author': author, 'ip_location': ip})
            bar.progress((idx + 1) / len(df))
            time.sleep(0.5)
        detail_df = pd.DataFrame(details)
        df = pd.concat([df, detail_df], axis=1)
        bar.empty()
    else:
        df['content'] = ""
        df['author'] = "æœªé‡‡é›†"
    return df

# --- æ ¸å¿ƒï¼šPlaywright è‡ªåŠ¨ç™»å½•é€»è¾‘ ---
def auto_login_playwright():
    """
    ä½¿ç”¨ Playwright å¯åŠ¨æµè§ˆå™¨å¹¶ç›‘å¬ç™»å½•çŠ¶æ€
    """
    status_placeholder = st.empty()
    token = None
    cookie_string = None
    
    try:
        status_placeholder.info("ğŸš€ æ­£åœ¨å¯åŠ¨ Chromium æµè§ˆå™¨...")
        
        with sync_playwright() as p:
            # 1. å¯åŠ¨æµè§ˆå™¨ (headless=False ä»¥ä¾¿çœ‹åˆ°ç•Œé¢æ‰«ç )
            browser = p.chromium.launch(headless=False)
            context = browser.new_context()
            page = context.new_page()

            status_placeholder.info("ğŸ”— æ­£åœ¨æ‰“å¼€å¾®ä¿¡ç™»å½•é¡µ...")
            page.goto("https://mp.weixin.qq.com/")
            
            status_placeholder.warning("ğŸ“± è¯·æ‹¿èµ·æ‰‹æœºå¾®ä¿¡æ‰«ç ç™»å½• (è¯·å‹¿å…³é—­æµè§ˆå™¨)...")

            # 2. å¾ªç¯æ£€æµ‹ URL Token
            max_retries = 120  # ç­‰å¾… 120 ç§’
            for i in range(max_retries):
                # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦è¢«æ‰‹åŠ¨å…³é—­
                if page.is_closed():
                    status_placeholder.error("æµè§ˆå™¨å·²å…³é—­ï¼Œæ“ä½œå–æ¶ˆã€‚")
                    return None, None
                    
                current_url = page.url
                if "token=" in current_url:
                    status_placeholder.success(f"âœ… ç™»å½•æˆåŠŸï¼æ­£åœ¨æå–å‡­è¯... ({i}s)")
                    
                    # A. æå– Token
                    parsed_url = urlparse(current_url)
                    params = parse_qs(parsed_url.query)
                    token = params.get("token", [""])[0]
                    
                    # B. æå– Cookies
                    cookies_list = context.cookies()
                    cookie_string = "; ".join([f"{cookie['name']}={cookie['value']}" for cookie in cookies_list])
                    
                    # ç¨ç­‰ç‰‡åˆ»ç¡®ä¿æ•°æ®ç¨³å®š
                    time.sleep(1)
                    break
                else:
                    time.sleep(1)
            
            if not token:
                status_placeholder.error("â° ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚")
            
            browser.close()
            
    except Exception as e:
        status_placeholder.error(f"Playwright å¯åŠ¨å¤±è´¥: {str(e)}")
        st.markdown("ğŸ’¡ **æç¤º**: ç¬¬ä¸€æ¬¡ä½¿ç”¨è¯·ç¡®ä¿å·²è¿è¡Œå‘½ä»¤å®‰è£…æµè§ˆå™¨å†…æ ¸:\n`playwright install`")
        return None, None
        
    return token, cookie_string

# --- ä¸»ç¨‹åº UI é€»è¾‘ ---

# åˆå§‹åŒ– session state
if 'wx_token' not in st.session_state:
    st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state:
    st.session_state['wx_cookie'] = ''

with st.sidebar:
    st.title("ğŸ¤– è‡ªåŠ¨è·å–åŠ©æ‰‹")
    st.caption("åŸºäº Playwright (Chromium)")

    # è‡ªåŠ¨è·å–æŒ‰é’®
    if st.button("ğŸ“¢ å”¤èµ·æµè§ˆå™¨æ‰«ç ", type="primary"):
        token, cookie = auto_login_playwright()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.balloons()
            st.success("å‡­è¯å·²è‡ªåŠ¨å¡«å…¥ï¼")
            
            # è‡ªåŠ¨å¤‡ä»½åˆ°æ¡Œé¢ (å¯é€‰)
            try:
                home = os.path.expanduser("~")
                save_dir = os.path.join(home, "Desktop", "finance")
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)
                with open(os.path.join(save_dir, "weixin_config_backup.txt"), "w") as f:
                    f.write(f"Token:\n{token}\n\nCookie:\n{cookie}")
            except:
                pass 
                
            time.sleep(1)
            st.rerun()
    
    st.divider()
    
    with st.expander("ğŸ”‘ å‡­è¯é…ç½®", expanded=True):
        wx_token = st.text_input("Token", value=st.session_state['wx_token'])
        wx_cookie = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=150)
    
    st.divider()
    target_query = st.text_input("ğŸ” ç›®æ ‡å…¬ä¼—å·", placeholder="è¾“å…¥åç§°")
    scrape_pages = st.number_input("æŠ“å–é¡µæ•°", 1, 10, 2)
    enable_details = st.checkbox("é‡‡é›†æ­£æ–‡ (é˜…è¯»æ¨¡å¼å¿…é€‰)", value=True)
    
    start_btn = st.button("ğŸš€ å¼€å§‹åˆ†ææ•°æ®", use_container_width=True)

# --- ä¸»ç•Œé¢ ---
if start_btn and wx_token and wx_cookie and target_query:
    crawler = WechatCrawler(wx_token, wx_cookie)
    
    with st.status("æ­£åœ¨å»ºç«‹æ•°æ®è¿æ¥...", expanded=True) as status:
        status.write("ğŸ” å®šä½ç›®æ ‡è´¦å·...")
        accounts = crawler.search_account(target_query)
        if not accounts:
            status.update(label="æœªæ‰¾åˆ°è´¦å·ï¼Œå¯èƒ½æ˜¯Cookieå·²å¤±æ•ˆï¼Œè¯·é‡æ–°æ‰«ç ", state="error")
            st.stop()
        
        target = accounts[0]
        status.write(f"âœ… é”å®š: {target['nickname']}")
        
        status.write("ğŸ“ƒ æ‹‰å–æ–‡ç« åˆ—è¡¨...")
        raw_list = crawler.fetch_article_list(target['fakeid'], pages=scrape_pages)
        
        if not raw_list:
             status.update(label="æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥å‡­è¯", state="error")
             st.stop()

        status.write("ğŸ§¹ æ·±åº¦é‡‡é›†æ­£æ–‡å†…å®¹...")
        df_res = process_data(raw_list, crawler, fetch_details=enable_details)
        
        status.update(label="æ•°æ®å‡†å¤‡å°±ç»ª!", state="complete")
        st.session_state['data'] = df_res
        st.session_state['account'] = target['nickname']

if 'data' in st.session_state:
    df = st.session_state['data']
    nickname = st.session_state['account']
    st.header(f"ğŸ“° {nickname} Â· æ·±åº¦é˜…è¯»çœ‹æ¿")
    
    tab_read, tab_list = st.tabs(["ğŸ‘“ é˜…è¯»æ¨¡å¼", "ğŸ“‹ æ–‡ç« åˆ—è¡¨"])
    
    with tab_read:
        if 'content' in df.columns and not df['content'].isna().all():
            df['select_label'] = df['date'].astype(str) + " | " + df['title']
            selected_article_label = st.selectbox("é€‰æ‹©æ–‡ç« :", df['select_label'].tolist())
            article = df[df['select_label'] == selected_article_label].iloc[0]
            
            with st.container():
                st.markdown(f"## {article['title']}")
                st.caption(f"ä½œè€…: {article['author']} | å‘å¸ƒæ—¶é—´: {article['publish_time']} | {article['is_original']}")
                st.divider()
                if article['content']:
                    st.markdown(article['content'].replace("\n", "\n\n"))
                else:
                    st.warning("æ­£æ–‡å†…å®¹ä¸ºç©º")
                    st.markdown(f"[ç‚¹å‡»è·³è½¬åŸæ–‡é“¾æ¥]({article['link']})")
        else:
            st.info("æš‚æ— æ­£æ–‡æ•°æ®ï¼Œè¯·ç¡®ä¿å‹¾é€‰äº†ã€é‡‡é›†æ­£æ–‡ã€‘å¹¶é‡æ–°æŠ“å–ã€‚")
            
    with tab_list:
        st.dataframe(
            df[['title', 'date', 'author', 'is_original', 'link']],
            use_container_width=True,
            column_config={"link": st.column_config.LinkColumn("åŸæ–‡é“¾æ¥")}
        )
else:
    st.info("ğŸ‘ˆ ç‚¹å‡»å·¦ä¾§ **'å”¤èµ·æµè§ˆå™¨æ‰«ç '** å¼€å§‹ã€‚")

import streamlit as st
import pandas as pd
import requests
import time
import random
import os
import sys
import subprocess
import jieba
import matplotlib.pyplot as plt
import collections
from wordcloud import WordCloud
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from playwright.sync_api import sync_playwright
from io import BytesIO
import base64

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="é«˜æ ¡å…¬ä¼—å·èˆ†æƒ…åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è§£å†³ Matplotlib ä¸­æ–‡ä¹±ç  (å°½å¯èƒ½å°è¯•å¤šç§å­—ä½“)
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'PingFang SC', 'Heiti TC', 'Microsoft YaHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# æ ¸å¿ƒå·¥å…·ç±»
# ==========================================

def get_chinese_font_path():
    """
    å°è¯•è·å–ç³»ç»Ÿä¸­çš„ä¸­æ–‡å­—ä½“è·¯å¾„ï¼Œç”¨äº WordCloud
    """
    system = sys.platform
    font_paths = []
    
    if system == "darwin": # MacOS
        font_paths = [
            "/System/Library/Fonts/PingFang.ttc",
            "/System/Library/Fonts/STHeiti Light.ttc",
            "/System/Library/Fonts/Supplemental/Arial Unicode.ttf"
        ]
    elif system == "win32": # Windows
        font_paths = [
            "C:\\Windows\\Fonts\\simhei.ttf",
            "C:\\Windows\\Fonts\\msyh.ttc",
            "C:\\Windows\\Fonts\\simsun.ttc"
        ]
    else: # Linux (Streamlit Cloud)
        font_paths = [
            "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
            "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
        ]
        
    for path in font_paths:
        if os.path.exists(path):
            return path
            
    return None # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¯äº‘å¯èƒ½ä¼šæ˜¾ç¤ºæ–¹æ¡†ï¼Œä½†ä¸ä¼šæŠ¥é”™

def clean_wechat_html(html_content):
    """
    [Bugä¿®å¤] æ·±åº¦æ¸…æ´—å¾®ä¿¡HTMLï¼Œç¡®ä¿å›¾ç‰‡æ˜¾ç¤ºå’Œæ’ç‰ˆæ­£å¸¸
    """
    if not html_content:
        return "<div style='padding:20px; text-align:center; color:#999'>ğŸ“­ æ­£æ–‡å†…å®¹ä¸ºç©º</div>"
    
    soup = BeautifulSoup(html_content, "html.parser")
    
    # 1. ç§»é™¤ script æ ‡ç­¾ï¼Œé˜²æ­¢æ‰§è¡Œæ¶æ„ä»£ç 
    for script in soup(["script", "style"]):
        script.decompose()

    # 2. ç ´è§£å›¾ç‰‡é˜²ç›—é“¾ & ä¿®å¤æ‡’åŠ è½½ (å…³é”®æ­¥éª¤)
    for img in soup.find_all("img"):
        # å¾®ä¿¡å›¾ç‰‡é€šå¸¸æ”¾åœ¨ data-src ä¸­
        if "data-src" in img.attrs:
            img["src"] = img["data-src"]
        
        # å¿…é¡»æ·»åŠ  no-referrerï¼Œå¦åˆ™å¾®ä¿¡æœåŠ¡å™¨ä¼šè¿”å› 403 Forbidden (è£‚å›¾)
        img["referrerpolicy"] = "no-referrer"
        
        # å¼ºåˆ¶æ ·å¼ï¼šè‡ªé€‚åº”å®½åº¦ï¼Œå±…ä¸­
        img["style"] = "max-width: 100% !important; height: auto !important; display: block; margin: 15px auto; border-radius: 6px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);"

    # 3. ä¼˜åŒ–æ’ç‰ˆå®¹å™¨
    # æ³¨å…¥ä¸€ä¸ªåŸºç¡€æ ·å¼ï¼Œæ¨¡æ‹Ÿå¾®ä¿¡é˜…è¯»ä½“éªŒ
    wrapper = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style>
            body {{
                font-family: -apple-system, BlinkMacSystemFont, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Arial, sans-serif;
                line-height: 1.8;
                color: #333;
                background-color: #fff;
                margin: 0;
                padding: 10px;
                font-size: 16px;
                text-align: justify;
            }}
            p {{ margin-bottom: 20px; }}
            strong {{ color: #000; font-weight: 700; }}
            blockquote {{
                border-left: 4px solid #07c160;
                background-color: #f8f8f8;
                margin: 20px 0;
                padding: 15px;
                color: #666;
            }}
        </style>
    </head>
    <body>
        <div id="js_content">
            {str(soup)}
        </div>
    </body>
    </html>
    """
    return wrapper

def generate_wordcloud_img(text_data):
    """
    ç”Ÿæˆè¯äº‘å›¾ç‰‡å¯¹è±¡
    """
    if not text_data:
        return None, []
        
    font_path = get_chinese_font_path()
    
    # ä½¿ç”¨ jieba åˆ†è¯
    words = jieba.cut(text_data)
    # è¿‡æ»¤åœç”¨è¯ (è¿™é‡Œç®€å•è¿‡æ»¤å•å­—å’Œå¸¸è§è™šè¯)
    filtered_words = [w for w in words if len(w) > 1 and w not in ['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'åœ¨', 'ä¸º', 'å¯¹', 'ç­‰', 'ç¯‡', 'å¾®', 'ä¿¡', 'å·', 'æœˆ', 'æ—¥', 'å¹´', 'æœ‰', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£']]
    space_split_text = " ".join(filtered_words)
    
    if not space_split_text.strip():
        return None, []

    try:
        wc = WordCloud(
            font_path=font_path,
            width=800,
            height=400,
            background_color='white',
            max_words=100,
            colormap='viridis',
            prefer_horizontal=0.9
        ).generate(space_split_text)
        
        return wc, filtered_words
    except Exception as e:
        print(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
        return None, []

class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def check_auth(self):
        url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": "test", "begin": "0", "count": "1"
        }
        try:
            res = self.session.get(url, params=params)
            data = res.json()
            if "base_resp" in data and data["base_resp"]["ret"] != 0:
                return False, f"Token å¤±æ•ˆæˆ– Cookie è¿‡æœŸ: {data['base_resp']}"
            return True, "éªŒè¯é€šè¿‡"
        except:
            return False, "ç½‘ç»œè¿æ¥å¼‚å¸¸"

    def search_account(self, query):
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params, timeout=10)
            data = res.json()
            return data.get("list", [])
        except:
            return []

    def fetch_article_list(self, fakeid, pages=1):
        all_articles = []
        for page in range(pages):
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params, timeout=10)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                time.sleep(random.uniform(1.0, 2.0))
            except:
                break
        return all_articles

    def fetch_content(self, url):
        try:
            res = self.session.get(url, timeout=15)
            # ä½¿ç”¨ html.parser å…¼å®¹æ€§æ›´å¥½
            soup = BeautifulSoup(res.text, "html.parser")
            
            # å°è¯•è·å–æ­£æ–‡å®¹å™¨ï¼Œå¾®ä¿¡é€šå¸¸æ˜¯ js_content
            content_div = soup.find("div", {"id": "js_content"}) or soup.find("div", {"class": "rich_media_content"})
            
            if content_div:
                final_html = clean_wechat_html(str(content_div))
                
                # æå–çº¯æ–‡æœ¬ç”¨äºè¯äº‘åˆ†æ
                plain_text = content_div.get_text(strip=True)
            else:
                final_html = "<div>è§£æå¤±è´¥ï¼Œå¯èƒ½æ–‡ç« å·²åˆ é™¤æˆ–éœ€è¦ç‰¹æ®Šæƒé™</div>"
                plain_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"}) or soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return final_html, author, plain_text
        except Exception:
            return "", "è·å–å¤±è´¥", ""

# ==========================================
# è‡ªåŠ¨åŒ–ç™»å½•æ¨¡å— (ç¡®ä¿æ— å¤´æ¨¡å¼)
# ==========================================

def force_install_chromium():
    try:
        cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except:
        return False

def auto_login_browser():
    status_box = st.empty()
    qr_box = st.empty()
    token = None
    cookie_str = None

    status_box.info("ğŸš€ æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–å¼•æ“ (é«˜æ¸…äº‘ç«¯æ¨¡å¼)...")

    try:
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
            except Exception as e:
                if "Executable doesn't exist" in str(e):
                    status_box.warning("âš™ï¸ æ­£åœ¨å®‰è£…æµè§ˆå™¨å†…æ ¸...")
                    if force_install_chromium():
                         status_box.success("âœ… å®‰è£…æˆåŠŸï¼é‡è¯•ä¸­...")
                         browser = p.chromium.launch(headless=True)
                    else:
                         return None, None
                else:
                    raise e

            context = browser.new_context(viewport={'width': 1920, 'height': 1080})
            page = context.new_page()

            status_box.info("ğŸ”— æ­£åœ¨åŠ è½½å¾®ä¿¡ç™»å½•é¡µ...")
            page.goto("https://mp.weixin.qq.com/")
            
            try:
                page.wait_for_selector(".login__type__container__scan", timeout=15000)
            except:
                pass 

            status_box.warning("ğŸ“± è¯·ä½¿ç”¨æ‰‹æœºå¾®ä¿¡æ‰«ç  (äºŒç»´ç å·²æ”¾å¤§):")
            
            max_wait = 120
            for i in range(max_wait):
                try:
                    if page.is_closed(): return None, None
                    current_url = page.url
                except: return None, None

                if i % 1.5 == 0 and "token=" not in current_url:
                    try:
                        qr_elem = page.locator(".login__type__container__scan")
                        if qr_elem.count() > 0:
                            screenshot_bytes = qr_elem.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å®æ—¶ç”»é¢)", width=400)
                        else:
                            screenshot_bytes = page.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å…¨å±å¤‡ç”¨)", width=600)
                    except:
                        pass

                if "token=" in current_url:
                    qr_box.empty()
                    status_box.success(f"âœ… ç™»å½•æˆåŠŸï¼")
                    
                    parsed = urlparse(current_url)
                    token = parse_qs(parsed.query).get("token", [""])[0]
                    cookies = context.cookies()
                    cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                    time.sleep(1)
                    break
                else:
                    time.sleep(1)
            
            browser.close()
            
    except Exception as e:
        status_box.error(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        return None, None

    return token, cookie_str

# ==========================================
# Streamlit ä¸»ç•Œé¢
# ==========================================

if 'wx_token' not in st.session_state: st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state: st.session_state['wx_cookie'] = ''
if 'all_data' not in st.session_state: st.session_state['all_data'] = None

with st.sidebar:
    st.title("ğŸ“ é«˜æ ¡èˆ†æƒ…åˆ†æ Pro")
    st.caption("Playwright é©±åŠ¨ Â· Jieba åˆ†è¯ Â· å¯è§†åŒ–")
    st.markdown("---")
    
    # ç™»å½•åŒº
    if st.button("ğŸ“¢ 1. æ‰«ç è·å–æƒé™", type="primary", use_container_width=True):
        token, cookie = auto_login_browser()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.success("æƒé™è·å–æˆåŠŸï¼")
            time.sleep(1)
            st.rerun()

    with st.expander("ğŸ”‘ å‡­è¯ç®¡ç†", expanded=True):
        token_input = st.text_input("Token", value=st.session_state['wx_token'])
        cookie_input = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=100)
        
        if token_input != st.session_state['wx_token']: st.session_state['wx_token'] = token_input
        if cookie_input != st.session_state['wx_cookie']: st.session_state['wx_cookie'] = cookie_input

    st.markdown("---")
    # è®¾ç½®åŒº
    targets_input = st.text_area(
        "2. è¾“å…¥é«˜æ ¡å…¬ä¼—å· (ä¸€è¡Œä¸€ä¸ª)", 
        placeholder="æ¸…åå¤§å­¦\nåŒ—äº¬å¤§å­¦\nå¤æ—¦å¤§å­¦",
        height=150
    )
    
    page_count = st.slider("æŠ“å–é¡µæ•° (æ¯é¡µ5ç¯‡)", 1, 10, 2)
    run_btn = st.button("ğŸš€ 3. å¼€å§‹æŠ“å–ä¸åˆ†æ", use_container_width=True)

# --- ä¸»é€»è¾‘åŒº ---

if run_btn:
    if not token_input or not cookie_input:
        st.error("è¯·å…ˆè·å– Token å’Œ Cookieï¼")
        st.stop()
    if not targets_input.strip():
        st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·ï¼")
        st.stop()
        
    target_list = [line.strip() for line in targets_input.split('\n') if line.strip()]
    crawler = WechatCrawler(token_input, cookie_input)
    
    all_results = []
    
    # é‡‡é›†è¿›åº¦
    status_container = st.status("æ­£åœ¨è¿›è¡Œå¤šæ ¡æ•°æ®é‡‡é›†...", expanded=True)
    progress_bar = st.progress(0)
    
    with status_container:
        # éªŒè¯
        if not crawler.check_auth()[0]:
            st.error("æƒé™éªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°æ‰«ç ï¼")
            st.stop()
            
        total_targets = len(target_list)
        for i, target_name in enumerate(target_list):
            st.write(f"ğŸ”„ [{i+1}/{total_targets}] åˆ†æ: **{target_name}** ...")
            
            # æœç´¢
            accounts = crawler.search_account(target_name)
            if not accounts:
                st.warning(f"âš ï¸ æœªæ‰¾åˆ°: {target_name}ï¼Œè·³è¿‡")
                continue
            
            target_account = accounts[0]
            fakeid = target_account['fakeid']
            real_nickname = target_account['nickname']
            
            # åˆ—è¡¨
            articles = crawler.fetch_article_list(fakeid, pages=page_count)
            
            # æ­£æ–‡è¯¦æƒ… (ç”¨äºè¯äº‘)
            if articles:
                st.write(f"   - æŠ“å–æ­£æ–‡ ({len(articles)}ç¯‡)...")
                for art in articles:
                    html_content, author, plain_text = crawler.fetch_content(art['link'])
                    art['content_html'] = html_content # ç”¨äºæ˜¾ç¤º
                    art['plain_text'] = plain_text # ç”¨äºåˆ†è¯
                    art['author'] = author
                    # è¡¥å……å…ƒæ•°æ®
                    art['account_name'] = real_nickname
                    time.sleep(0.5)

            all_results.extend(articles)
            progress_bar.progress((i + 1) / total_targets)
            time.sleep(random.uniform(1.5, 3.0))
            
        status_container.update(label="âœ… é‡‡é›†ä¸åˆ†æå®Œæˆï¼", state="complete")
    
    # å­˜å…¥ Session
    if all_results:
        df = pd.DataFrame(all_results)
        df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['create_time'], unit='s')
        df['å‘å¸ƒæ—¥æœŸ'] = df['å‘å¸ƒæ—¶é—´'].dt.date
        df['ç±»å‹'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
        st.session_state['all_data'] = df
    else:
        st.warning("æœªé‡‡é›†åˆ°æ•°æ®ã€‚")

# --- åˆ†æçœ‹æ¿ ---

if st.session_state['all_data'] is not None:
    df = st.session_state['all_data']
    
    st.divider()
    st.title("ğŸ“Š é«˜æ ¡æ–°åª’ä½“å¤§æ•°æ®çœ‹æ¿")
    
    # ----------------------------------------------------
    # 1. å®è§‚æ•°æ®åˆ†æ (æ‰€æœ‰å­¦æ ¡)
    # ----------------------------------------------------
    st.header("1. å…¨ç½‘ç»¼åˆèˆ†æƒ… (All Schools)")
    
    tab_global_1, tab_global_2, tab_global_3 = st.tabs(["â˜ï¸ ç»¼åˆè¯äº‘", "ğŸ† å½±å“åŠ›æ’è¡Œ", "ğŸ“ˆ å‘æ–‡è¶‹åŠ¿"])
    
    with tab_global_1:
        col_g1, col_g2 = st.columns(2)
        with col_g1:
            st.subheader("å…¨ç½‘Â·æ ‡é¢˜è¯äº‘")
            all_titles = " ".join(df['title'].tolist())
            wc_title, _ = generate_wordcloud_img(all_titles)
            if wc_title:
                st.image(wc_title.to_array(), use_container_width=True)
            else:
                st.info("æ•°æ®ä¸è¶³ç”Ÿæˆè¯äº‘")
                
        with col_g2:
            st.subheader("å…¨ç½‘Â·å†…å®¹è¯äº‘")
            all_contents = " ".join(df['plain_text'].fillna("").tolist())
            wc_content, words_list = generate_wordcloud_img(all_contents)
            if wc_content:
                st.image(wc_content.to_array(), use_container_width=True)
                
            # å…¨ç½‘ TOP 10 å…³é”®è¯
            if words_list:
                st.caption("ğŸ”¥ å…¨ç½‘ TOP 10 çƒ­è¯:")
                counts = collections.Counter(words_list)
                top10 = counts.most_common(10)
                st.write(" | ".join([f"**{w}**({c})" for w, c in top10]))

    with tab_global_2:
        st.subheader("é«˜æ ¡æ´»è·ƒåº¦æ’è¡Œæ¦œ (æŒ‰å‘æ–‡é‡)")
        st.caption("æ³¨ï¼šå¾®ä¿¡PCæ¥å£æ— æ³•è·å–ç«å“æ–‡ç« çš„é˜…è¯»é‡/ç‚¹èµæ•°ï¼Œæ•…æ­¤å¤„å±•ç¤ºã€å‘æ–‡æ´»è·ƒåº¦ã€‘æ’è¡Œã€‚")
        
        # æœ¬å‘¨/æœ¬æœˆè®¡ç®—
        now = pd.Timestamp.now()
        one_week_ago = now - pd.Timedelta(days=7)
        one_month_ago = now - pd.Timedelta(days=30)
        
        df['dt'] = pd.to_datetime(df['create_time'], unit='s')
        
        col_r1, col_r2 = st.columns(2)
        with col_r1:
            st.markdown("#### ğŸ“… æœ¬å‘¨å‘æ–‡æ¦œ")
            week_df = df[df['dt'] > one_week_ago]
            if not week_df.empty:
                week_rank = week_df['account_name'].value_counts().reset_index()
                week_rank.columns = ['é«˜æ ¡åç§°', 'å‘æ–‡æ•°']
                st.dataframe(week_rank, use_container_width=True, hide_index=True)
            else:
                st.info("æœ¬å‘¨æ— å‘æ–‡")
                
        with col_r2:
            st.markdown("#### ğŸ—“ï¸ æœ¬æœˆå‘æ–‡æ¦œ")
            month_df = df[df['dt'] > one_month_ago]
            if not month_df.empty:
                month_rank = month_df['account_name'].value_counts().reset_index()
                month_rank.columns = ['é«˜æ ¡åç§°', 'å‘æ–‡æ•°']
                st.dataframe(month_rank, use_container_width=True, hide_index=True)
            else:
                st.info("æœ¬æœˆæ— å‘æ–‡")

    with tab_global_3:
        st.subheader("å…¨ç½‘å‘å¸ƒæ—¶é—´åˆ†å¸ƒ")
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        date_counts = df.groupby('å‘å¸ƒæ—¥æœŸ').size()
        st.line_chart(date_counts)

    st.markdown("---")

    # ----------------------------------------------------
    # 2. ä¸ªä½“ç”»åƒåˆ†æ (æ¯ä¸ªå­¦æ ¡)
    # ----------------------------------------------------
    st.header("2. å•æ ¡æ·±åº¦ç”»åƒ (Single School)")
    
    school_list = df['account_name'].unique()
    selected_school = st.selectbox("ğŸ‘‰ é€‰æ‹©ä¸€æ‰€é«˜æ ¡æŸ¥çœ‹è¯¦æƒ…:", school_list)
    
    if selected_school:
        sub_df = df[df['account_name'] == selected_school]
        
        # 2.1 ç»Ÿè®¡æŒ‡æ ‡
        c1, c2, c3 = st.columns(3)
        c1.metric("æ€»å‘æ–‡æ•°", len(sub_df))
        c2.metric("åŸåˆ›æ¯”ä¾‹", f"{len(sub_df[sub_df['ç±»å‹']=='åŸåˆ›']) / len(sub_df) * 100:.1f}%" if len(sub_df)>0 else "0%")
        c3.metric("æœ€æ–°å‘å¸ƒ", str(sub_df['å‘å¸ƒæ—¥æœŸ'].max()))
        
        # 2.2 è¯äº‘ä¸TOP10
        tab_s1, tab_s2, tab_s3 = st.tabs(["â˜ï¸ ä¸“å±è¯äº‘", "ğŸ“Š æ–‡ç« åˆ—è¡¨", "ğŸ‘“ é˜…è¯»æ­£æ–‡"])
        
        with tab_s1:
            sc1, sc2 = st.columns(2)
            with sc1:
                st.markdown("**æ ‡é¢˜è¯äº‘**")
                s_titles = " ".join(sub_df['title'].tolist())
                s_wc_t, _ = generate_wordcloud_img(s_titles)
                if s_wc_t: st.image(s_wc_t.to_array(), use_container_width=True)
                
            with sc2:
                st.markdown("**å†…å®¹è¯äº‘**")
                s_content = " ".join(sub_df['plain_text'].fillna("").tolist())
                s_wc_c, s_words = generate_wordcloud_img(s_content)
                if s_wc_c: 
                    st.image(s_wc_c.to_array(), use_container_width=True)
                    st.markdown("---")
                    # TOP 10
                    s_counts = collections.Counter(s_words)
                    s_top10 = s_counts.most_common(10)
                    st.write("ğŸ”¥ **æ ¡å†…TOP10çƒ­è¯:**")
                    st.json(dict(s_top10))
        
        with tab_s2:
            st.dataframe(
                sub_df[['title', 'å‘å¸ƒæ—¶é—´', 'ç±»å‹', 'digest']], 
                use_container_width=True
            )
            
        with tab_s3:
            # é˜…è¯»å™¨
            if 'content_html' in sub_df.columns:
                read_idx = st.selectbox("é€‰æ‹©æ–‡ç« é˜…è¯»:", sub_df.index, format_func=lambda x: sub_df.loc[x, 'title'])
                read_art = sub_df.loc[read_idx]
                
                with st.container(border=True):
                    st.markdown(f"### {read_art['title']}")
                    st.caption(f"ä½œè€…: {read_art['author']} | æ—¶é—´: {read_art['å‘å¸ƒæ—¶é—´']}")
                    st.components.v1.html(read_art['content_html'], height=800, scrolling=True)
            else:
                st.warning("æ— æ­£æ–‡æ•°æ®")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ è¿›è¡Œæ“ä½œï¼šæ‰«ç  -> è¾“å…¥é«˜æ ¡åç§° -> å¼€å§‹åˆ†æ")

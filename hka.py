import streamlit as st
import pandas as pd
import requests
import time
import random
import os
import sys
import subprocess
import jieba
import matplotlib.pyplot as plt
import collections
import re
from wordcloud import WordCloud
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from playwright.sync_api import sync_playwright
from io import BytesIO
import base64

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="å…¬ä¼—å·èˆ†æƒ…åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# å­—ä½“ç®¡ç† (è§£å†³ä¸­æ–‡ä¹±ç )
# ==========================================

def get_font_path():
    """
    è·å–ä¸­æ–‡å­—ä½“è·¯å¾„ã€‚å¦‚æœç³»ç»Ÿæ²¡æœ‰ï¼Œå°è¯•ä¸‹è½½ SimHeiã€‚
    """
    # 1. ä¼˜å…ˆæ£€æŸ¥å½“å‰ç›®å½•ä¸‹æ˜¯å¦æœ‰å­—ä½“æ–‡ä»¶
    local_font = "SimHei.ttf"
    if os.path.exists(local_font):
        return local_font
    
    # 2. æ£€æŸ¥å¸¸è§ç³»ç»Ÿè·¯å¾„
    system_fonts = [
        "/System/Library/Fonts/PingFang.ttc", # MacOS
        "/System/Library/Fonts/STHeiti Light.ttc",
        "C:\\Windows\\Fonts\\simhei.ttf", # Windows
        "C:\\Windows\\Fonts\\msyh.ttc", 
        "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc", # Linux
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
    ]
    for path in system_fonts:
        if os.path.exists(path):
            return path
            
    # 3. å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸‹è½½ (é’ˆå¯¹ Streamlit Cloud)
    st.toast("æ­£åœ¨ä¸‹è½½ä¸­æ–‡å­—ä½“ï¼Œè¯·ç¨å€™...", icon="ğŸ“¥")
    try:
        url = "https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf"
        res = requests.get(url, timeout=30)
        with open(local_font, "wb") as f:
            f.write(res.content)
        return local_font
    except:
        pass
        
    return None

# è®¾ç½® Matplotlib å­—ä½“
font_path = get_font_path()
if font_path:
    # æ³¨å†Œå­—ä½“ç»™ matplotlib
    import matplotlib.font_manager as fm
    fe = fm.FontEntry(fname=font_path, name='CustomFont')
    fm.fontManager.ttflist.insert(0, fe)
    plt.rcParams['font.sans-serif'] = ['CustomFont']
else:
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# æ ¸å¿ƒå·¥å…·ç±»
# ==========================================

def clean_wechat_html(html_content):
    """
    [ä¿®å¤ç‰ˆ] æ·±åº¦æ¸…æ´—å¾®ä¿¡HTMLï¼Œå¼ºåˆ¶æ˜¾ç¤ºå†…å®¹
    """
    if not html_content:
        return "<div style='padding:20px; text-align:center; color:#999'>ğŸ“­ æ­£æ–‡å†…å®¹ä¸ºç©º</div>"
    
    soup = BeautifulSoup(html_content, "html.parser")
    
    # 1. å…³é”®ä¿®å¤ï¼šç§»é™¤ visibility: hidden
    # å¾®ä¿¡æ­£æ–‡ div (js_content) é»˜è®¤æ˜¯éšè—çš„ï¼Œä¾èµ– JS æ˜¾ç¤ºã€‚æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¼ºåˆ¶æ˜¾ç¤ºã€‚
    content_div = soup.find("div", id="js_content")
    if content_div:
        # ç§»é™¤åŸæœ‰ styleï¼Œæˆ–è€…å¼ºåˆ¶è¦†ç›–
        existing_style = content_div.get('style', '')
        content_div['style'] = existing_style + '; visibility: visible !important; opacity: 1 !important;'
    
    # 2. ç§»é™¤å¹²æ‰°è„šæœ¬
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()

    # 3. ç ´è§£å›¾ç‰‡é˜²ç›—é“¾
    for img in soup.find_all("img"):
        if "data-src" in img.attrs:
            img["src"] = img["data-src"]
        img["referrerpolicy"] = "no-referrer"
        img["style"] = "max-width: 100% !important; height: auto !important; display: block; margin: 10px auto; border-radius: 4px;"

    # 4. åŒ…è£…å®¹å™¨
    wrapper = f"""
    <div style="
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        line-height: 1.8;
        color: #333;
        font-size: 16px;
        background-color: #fff;
        padding: 15px;
    ">
        {str(soup)}
    </div>
    """
    return wrapper

def generate_wordcloud_img(text_data):
    """
    ç”Ÿæˆè¯äº‘å›¾ç‰‡å¯¹è±¡
    """
    if not text_data:
        return None, []
        
    f_path = get_font_path()
    
    # ä½¿ç”¨ jieba åˆ†è¯
    words = jieba.lcut(text_data)
    
    # æ‰©å±•åœç”¨è¯è¡¨
    stop_words = set([
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'åœ¨', 'ä¸º', 'å¯¹', 'ç­‰', 'ç¯‡', 
        'å¾®', 'ä¿¡', 'å·', 'æœˆ', 'æ—¥', 'å¹´', 'æœ‰', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£',
        'æˆ‘ä»¬', 'å›¾ç‰‡', 'æ¥æº', 'åŸæ ‡', 'é¢˜', 'å…¬ä¼—', 'ç‚¹å‡»', 'é˜…è¯»', 'åŸæ–‡', 'ä¸‹æ–¹', 'å…³æ³¨',
        'å±•å¼€', 'å…¨æ–‡', 'è§†é¢‘', 'åˆ†äº«', 'æ”¶è—', 'ç‚¹èµ', 'åœ¨çœ‹'
    ])
    
    filtered_words = [w for w in words if len(w) > 1 and w not in stop_words]
    space_split_text = " ".join(filtered_words)
    
    if not space_split_text.strip():
        return None, []

    try:
        wc = WordCloud(
            font_path=f_path,
            width=800,
            height=400,
            background_color='white',
            max_words=150,
            colormap='viridis',
            prefer_horizontal=0.9
        ).generate(space_split_text)
        
        return wc, filtered_words
    except Exception as e:
        print(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
        return None, []

class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def check_auth(self):
        url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": "test", "begin": "0", "count": "1"
        }
        try:
            res = self.session.get(url, params=params)
            data = res.json()
            if "base_resp" in data and data["base_resp"]["ret"] != 0:
                return False, f"Token å¤±æ•ˆæˆ– Cookie è¿‡æœŸ: {data['base_resp']}"
            return True, "éªŒè¯é€šè¿‡"
        except:
            return False, "ç½‘ç»œè¿æ¥å¼‚å¸¸"

    def search_account(self, query):
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params, timeout=10)
            data = res.json()
            return data.get("list", [])
        except:
            return []

    def fetch_article_list(self, fakeid, pages=1):
        all_articles = []
        for page in range(pages):
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params, timeout=10)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                time.sleep(random.uniform(0.5, 1.5))
            except:
                break
        return all_articles

    def fetch_content(self, url):
        try:
            res = self.session.get(url, timeout=15)
            # ä½¿ç”¨ html.parser
            soup = BeautifulSoup(res.text, "html.parser")
            
            # æŸ¥æ‰¾æ­£æ–‡ (js_content)
            content_div = soup.find("div", id="js_content")
            
            if content_div:
                final_html = clean_wechat_html(str(soup)) # ä¼ å…¥æ•´ä¸ªsoupè®©cleanå‡½æ•°å¤„ç†
                plain_text = content_div.get_text(strip=True)
            else:
                final_html = clean_wechat_html(res.text) # å¤‡ç”¨ï¼šç›´æ¥ä¼ åŸæ–‡
                plain_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"}) or soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return final_html, author, plain_text
        except Exception:
            return "<div>è·å–å¤±è´¥</div>", "è·å–å¤±è´¥", ""

# ==========================================
# è‡ªåŠ¨åŒ–ç™»å½•æ¨¡å—
# ==========================================

def force_install_chromium():
    try:
        cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except:
        return False

def auto_login_browser():
    status_box = st.empty()
    qr_box = st.empty()
    token = None
    cookie_str = None

    status_box.info("ğŸš€ æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–å¼•æ“ (é«˜æ¸…äº‘ç«¯æ¨¡å¼)...")

    try:
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
            except Exception as e:
                if "Executable doesn't exist" in str(e):
                    status_box.warning("âš™ï¸ æ­£åœ¨å®‰è£…æµè§ˆå™¨å†…æ ¸...")
                    if force_install_chromium():
                         status_box.success("âœ… å®‰è£…æˆåŠŸï¼é‡è¯•ä¸­...")
                         browser = p.chromium.launch(headless=True)
                    else:
                         return None, None
                else:
                    raise e

            context = browser.new_context(viewport={'width': 1920, 'height': 1080})
            page = context.new_page()

            status_box.info("ğŸ”— æ­£åœ¨åŠ è½½å¾®ä¿¡ç™»å½•é¡µ...")
            page.goto("https://mp.weixin.qq.com/")
            
            try:
                page.wait_for_selector(".login__type__container__scan", timeout=15000)
            except:
                pass 

            status_box.warning("ğŸ“± è¯·ä½¿ç”¨æ‰‹æœºå¾®ä¿¡æ‰«ç  (äºŒç»´ç å·²æ”¾å¤§):")
            
            max_wait = 120
            for i in range(max_wait):
                try:
                    if page.is_closed(): return None, None
                    current_url = page.url
                except: return None, None

                if i % 1.5 == 0 and "token=" not in current_url:
                    try:
                        qr_elem = page.locator(".login__type__container__scan")
                        if qr_elem.count() > 0:
                            screenshot_bytes = qr_elem.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å®æ—¶ç”»é¢)", width=400)
                        else:
                            screenshot_bytes = page.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å…¨å±å¤‡ç”¨)", width=600)
                    except:
                        pass

                if "token=" in current_url:
                    qr_box.empty()
                    status_box.success(f"âœ… ç™»å½•æˆåŠŸï¼")
                    
                    parsed = urlparse(current_url)
                    token = parse_qs(parsed.query).get("token", [""])[0]
                    cookies = context.cookies()
                    cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                    time.sleep(1)
                    break
                else:
                    time.sleep(1)
            
            browser.close()
            
    except Exception as e:
        status_box.error(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        return None, None

    return token, cookie_str

# ==========================================
# Streamlit ä¸»ç•Œé¢
# ==========================================

if 'wx_token' not in st.session_state: st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state: st.session_state['wx_cookie'] = ''
if 'all_data' not in st.session_state: st.session_state['all_data'] = None

with st.sidebar:
    st.title("ğŸ“ å…¬ä¼—å·èˆ†æƒ…åˆ†æ Pro")
    st.caption("Playwright é©±åŠ¨ Â· è¯äº‘åˆ†æ Â· æ•°æ®å¯è§†åŒ–")
    st.markdown("---")
    
    # ç™»å½•åŒº
    if st.button("ğŸ“¢ 1. æ‰«ç è·å–æƒé™", type="primary", use_container_width=True):
        token, cookie = auto_login_browser()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.success("æƒé™è·å–æˆåŠŸï¼")
            time.sleep(1)
            st.rerun()

    with st.expander("ğŸ”‘ å‡­è¯ç®¡ç†", expanded=True):
        token_input = st.text_input("Token", value=st.session_state['wx_token'])
        cookie_input = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=100)
        
        if token_input != st.session_state['wx_token']: st.session_state['wx_token'] = token_input
        if cookie_input != st.session_state['wx_cookie']: st.session_state['wx_cookie'] = cookie_input

    st.markdown("---")
    # è®¾ç½®åŒº
    targets_input = st.text_area(
        "2. è¾“å…¥å…¬ä¼—å·åç§°", 
        placeholder="æ”¯æŒä¸­æ–‡é€—å·ã€é¡¿å·ã€ç©ºæ ¼æˆ–æ¢è¡Œåˆ†éš”\nä¾‹å¦‚ï¼š\næ¸…åå¤§å­¦ã€åŒ—äº¬å¤§å­¦ï¼Œå¤æ—¦å¤§å­¦",
        height=150
    )
    
    page_count = st.slider("æ¯ä¸ªå·æŠ“å–é¡µæ•° (æ¯é¡µ5ç¯‡)", 1, 5, 2)
    run_btn = st.button("ğŸš€ 3. å¼€å§‹åˆ†æ", use_container_width=True)

# --- ä¸»é€»è¾‘åŒº ---

if run_btn:
    if not token_input or not cookie_input:
        st.error("è¯·å…ˆè·å– Token å’Œ Cookieï¼")
        st.stop()
    if not targets_input.strip():
        st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·ï¼")
        st.stop()
        
    # æ™ºèƒ½åˆ†å‰²è¾“å…¥ï¼šæ”¯æŒä¸­æ–‡é€—å·ã€é¡¿å·ã€è‹±æ–‡é€—å·ã€ç©ºæ ¼ã€æ¢è¡Œ
    target_list = re.split(r'[,\s\nï¼Œã€]+', targets_input.strip())
    target_list = [t for t in target_list if t] # å»ç©º
    
    crawler = WechatCrawler(token_input, cookie_input)
    
    all_results = []
    status_container = st.status("æ­£åœ¨è¿›è¡Œæ•°æ®é‡‡é›†...", expanded=True)
    progress_bar = st.progress(0)
    
    with status_container:
        if not crawler.check_auth()[0]:
            st.error("æƒé™éªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°æ‰«ç ï¼")
            st.stop()
            
        total_targets = len(target_list)
        for i, target_name in enumerate(target_list):
            st.write(f"ğŸ”„ [{i+1}/{total_targets}] åˆ†æ: **{target_name}** ...")
            
            accounts = crawler.search_account(target_name)
            if not accounts:
                st.warning(f"âš ï¸ æœªæ‰¾åˆ°: {target_name}ï¼Œè·³è¿‡")
                continue
            
            target_account = accounts[0]
            fakeid = target_account['fakeid']
            real_nickname = target_account['nickname']
            
            articles = crawler.fetch_article_list(fakeid, pages=page_count)
            
            if articles:
                st.write(f"   - æŠ“å–æ­£æ–‡ ({len(articles)}ç¯‡)...")
                for art in articles:
                    html_content, author, plain_text = crawler.fetch_content(art['link'])
                    art['content_html'] = html_content
                    art['plain_text'] = plain_text
                    art['author'] = author
                    art['account_name'] = real_nickname
                    time.sleep(0.5)

            all_results.extend(articles)
            progress_bar.progress((i + 1) / total_targets)
            time.sleep(random.uniform(1.0, 2.0))
            
        status_container.update(label="âœ… é‡‡é›†ä¸åˆ†æå®Œæˆï¼", state="complete")
    
    if all_results:
        df = pd.DataFrame(all_results)
        df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['create_time'], unit='s')
        df['å‘å¸ƒæ—¥æœŸ'] = df['å‘å¸ƒæ—¶é—´'].dt.date
        df['ç±»å‹'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
        st.session_state['all_data'] = df
    else:
        st.warning("æœªé‡‡é›†åˆ°æ•°æ®ã€‚")

# --- åˆ†æçœ‹æ¿ ---

if st.session_state['all_data'] is not None:
    df = st.session_state['all_data']
    
    st.divider()
    st.title("ğŸ“Š å…¬ä¼—å·æ–°åª’ä½“å¤§æ•°æ®çœ‹æ¿")
    
    # 1. å®è§‚æ•°æ®
    st.header("1. å…¨ç½‘ç»¼åˆèˆ†æƒ… (All Accounts)")
    
    tab_global_1, tab_global_2, tab_global_3 = st.tabs(["â˜ï¸ ç»¼åˆè¯äº‘", "ğŸ† å½±å“åŠ›æ’è¡Œ", "ğŸ“ˆ å‘æ–‡è¶‹åŠ¿"])
    
    with tab_global_1:
        col_g1, col_g2 = st.columns(2)
        with col_g1:
            st.subheader("å…¨ç½‘Â·æ ‡é¢˜è¯äº‘")
            all_titles = " ".join(df['title'].tolist())
            wc_title, _ = generate_wordcloud_img(all_titles)
            if wc_title:
                st.image(wc_title.to_array(), use_container_width=True)
            else:
                st.info("æ•°æ®ä¸è¶³")
                
        with col_g2:
            st.subheader("å…¨ç½‘Â·å†…å®¹è¯äº‘")
            all_contents = " ".join(df['plain_text'].fillna("").tolist())
            wc_content, words_list = generate_wordcloud_img(all_contents)
            if wc_content:
                st.image(wc_content.to_array(), use_container_width=True)
            
            if words_list:
                st.markdown("**ğŸ”¥ å…¨ç½‘ TOP 10 çƒ­è¯:**")
                counts = collections.Counter(words_list)
                top10 = counts.most_common(10)
                # ä½¿ç”¨ DataFrame æ˜¾ç¤ºæ›´æ•´é½
                top10_df = pd.DataFrame(top10, columns=['å…³é”®è¯', 'é¢‘æ¬¡'])
                st.dataframe(top10_df.T, use_container_width=True)

    with tab_global_2:
        st.caption("æ³¨ï¼šæ•°æ®åŸºäºæœ¬æ¬¡æŠ“å–çš„æ ·æœ¬è®¡ç®—ã€‚")
        col_r1, col_r2 = st.columns(2)
        
        now = pd.Timestamp.now()
        
        with col_r1:
            st.markdown("#### ğŸ“… æœ¬å‘¨å‘æ–‡æ¦œ")
            week_df = df[df['å‘å¸ƒæ—¶é—´'] > (now - pd.Timedelta(days=7))]
            if not week_df.empty:
                week_rank = week_df['account_name'].value_counts().reset_index()
                week_rank.columns = ['å…¬ä¼—å·', 'å‘æ–‡æ•°']
                st.dataframe(week_rank, use_container_width=True, hide_index=True)
            else:
                st.info("æœ¬å‘¨æ— æ•°æ®")
                
        with col_r2:
            st.markdown("#### ğŸ—“ï¸ æœ¬æœˆå‘æ–‡æ¦œ")
            month_df = df[df['å‘å¸ƒæ—¶é—´'] > (now - pd.Timedelta(days=30))]
            if not month_df.empty:
                month_rank = month_df['account_name'].value_counts().reset_index()
                month_rank.columns = ['å…¬ä¼—å·', 'å‘æ–‡æ•°']
                st.dataframe(month_rank, use_container_width=True, hide_index=True)
            else:
                st.info("æœ¬æœˆæ— æ•°æ®")

    with tab_global_3:
        st.subheader("å…¨ç½‘å‘å¸ƒæ—¶é—´åˆ†å¸ƒ")
        date_counts = df.groupby('å‘å¸ƒæ—¥æœŸ').size()
        st.line_chart(date_counts)

    st.markdown("---")

    # 2. ä¸ªä½“ç”»åƒ
    st.header("2. å•å·æ·±åº¦ç”»åƒ (Single Account)")
    
    account_list = df['account_name'].unique()
    selected_account = st.selectbox("ğŸ‘‰ é€‰æ‹©ä¸€ä¸ªå…¬ä¼—å·æŸ¥çœ‹è¯¦æƒ…:", account_list)
    
    if selected_account:
        sub_df = df[df['account_name'] == selected_account].copy()
        
        # ç»Ÿè®¡æŒ‡æ ‡
        c1, c2, c3 = st.columns(3)
        c1.metric("æ€»å‘æ–‡æ•°", len(sub_df))
        c2.metric("åŸåˆ›æ¯”ä¾‹", f"{len(sub_df[sub_df['ç±»å‹']=='åŸåˆ›']) / len(sub_df) * 100:.1f}%" if len(sub_df)>0 else "0%")
        c3.metric("æœ€æ–°å‘å¸ƒ", str(sub_df['å‘å¸ƒæ—¥æœŸ'].max()))
        
        tab_s1, tab_s2 = st.tabs(["ğŸ“Š æ•°æ®åˆ†æ", "ğŸ“° æ–‡ç« åˆ—è¡¨ä¸é˜…è¯»"])
        
        with tab_s1:
            sc1, sc2 = st.columns(2)
            with sc1:
                st.markdown("**æ ‡é¢˜è¯äº‘**")
                s_titles = " ".join(sub_df['title'].tolist())
                s_wc_t, _ = generate_wordcloud_img(s_titles)
                if s_wc_t: st.image(s_wc_t.to_array(), use_container_width=True)
                
            with sc2:
                st.markdown("**å†…å®¹è¯äº‘**")
                s_content = " ".join(sub_df['plain_text'].fillna("").tolist())
                s_wc_c, s_words = generate_wordcloud_img(s_content)
                if s_wc_c: 
                    st.image(s_wc_c.to_array(), use_container_width=True)
                    if s_words:
                        st.markdown("**ğŸ”¥ TOP 10 çƒ­è¯:**")
                        s_counts = collections.Counter(s_words)
                        st.json(dict(s_counts.most_common(10)))

        with tab_s2:
            # å¸ƒå±€ä¼˜åŒ–ï¼šå·¦ä¾§åˆ—è¡¨ï¼Œå³ä¾§æ­£æ–‡
            col_list, col_read = st.columns([1, 2])
            
            with col_list:
                st.markdown("##### æ–‡ç« åˆ—è¡¨")
                # ä½¿ç”¨ Selectbox æ¨¡æ‹Ÿç‚¹å‡»è¿›å…¥
                # æ„é€ ä¸€ä¸ªæ˜¾ç¤ºç”¨çš„ Label
                sub_df['label'] = sub_df.apply(lambda x: f"{x['å‘å¸ƒæ—¥æœŸ']} | {x['title']}", axis=1)
                
                selected_article_label = st.radio(
                    "ç‚¹å‡»é€‰æ‹©æ–‡ç« é˜…è¯»:",
                    sub_df['label'].tolist(),
                    label_visibility="collapsed"
                )
            
            with col_read:
                if selected_article_label:
                    # æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« 
                    read_art = sub_df[sub_df['label'] == selected_article_label].iloc[0]
                    
                    st.markdown(f"#### {read_art['title']}")
                    st.caption(f"âœï¸ {read_art['author']}  |  ğŸ•’ {read_art['å‘å¸ƒæ—¶é—´']}")
                    st.divider()
                    
                    if 'content_html' in read_art and read_art['content_html']:
                        st.components.v1.html(read_art['content_html'], height=800, scrolling=True)
                    else:
                        st.warning("æ­£æ–‡å†…å®¹ä¸ºç©º")

else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ æ“ä½œï¼šæ‰«ç  -> è¾“å…¥å…¬ä¼—å· -> å¼€å§‹åˆ†æ")

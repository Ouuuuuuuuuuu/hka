import streamlit as st
import pandas as pd
import requests
import time
import random
import os
import sys
import subprocess
import jieba
import matplotlib.pyplot as plt
import collections
import re
import json
from wordcloud import WordCloud
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from playwright.sync_api import sync_playwright
from io import BytesIO
import base64

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="å…¬ä¼—å·èˆ†æƒ…åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# å­—ä½“ç®¡ç† (è§£å†³ä¸­æ–‡ä¹±ç )
# ==========================================

def get_font_path():
    """
    è·å–ä¸­æ–‡å­—ä½“è·¯å¾„ã€‚å¦‚æœç³»ç»Ÿæ²¡æœ‰ï¼Œå°è¯•ä¸‹è½½ SimHeiã€‚
    """
    local_font = "SimHei.ttf"
    if os.path.exists(local_font):
        return local_font
    
    system_fonts = [
        "/System/Library/Fonts/PingFang.ttc", # MacOS
        "/System/Library/Fonts/STHeiti Light.ttc",
        "C:\\Windows\\Fonts\\simhei.ttf", # Windows
        "C:\\Windows\\Fonts\\msyh.ttc", 
        "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc", # Linux
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
    ]
    for path in system_fonts:
        if os.path.exists(path):
            return path
            
    try:
        url = "https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf"
        res = requests.get(url, timeout=30)
        with open(local_font, "wb") as f:
            f.write(res.content)
        return local_font
    except:
        pass
        
    return None

font_path = get_font_path()
if font_path:
    import matplotlib.font_manager as fm
    fe = fm.FontEntry(fname=font_path, name='CustomFont')
    fm.fontManager.ttflist.insert(0, fe)
    plt.rcParams['font.sans-serif'] = ['CustomFont']
else:
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# æ ¸å¿ƒå·¥å…·ç±»
# ==========================================

def clean_wechat_html(html_content):
    if not html_content:
        return "<div style='padding:20px; text-align:center; color:#999'>ğŸ“­ æ­£æ–‡å†…å®¹ä¸ºç©º</div>"
    
    soup = BeautifulSoup(html_content, "html.parser")
    
    content_div = soup.find("div", id="js_content")
    if content_div:
        existing_style = content_div.get('style', '')
        content_div['style'] = existing_style + '; visibility: visible !important; opacity: 1 !important;'
    
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()

    for img in soup.find_all("img"):
        if "data-src" in img.attrs:
            img["src"] = img["data-src"]
        img["referrerpolicy"] = "no-referrer"
        img["style"] = "max-width: 100% !important; height: auto !important; display: block; margin: 10px auto; border-radius: 4px;"

    wrapper = f"""
    <div style="
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        line-height: 1.8;
        color: #333;
        font-size: 16px;
        background-color: #fff;
        padding: 15px;
    ">
        {str(soup)}
    </div>
    """
    return wrapper

def get_smart_stopwords(name_list):
    """
    æ™ºèƒ½ç”Ÿæˆå±è”½è¯ï¼š
    1. åŸºç¡€è‡ªç§° (æœ¬æ ¡, æˆ‘é™¢...)
    2. å…¨å & åˆ†è¯
    3. æ™ºèƒ½ç®€ç§° (å—å¤–, åŒ—å¤§, æ·±ä¸­, åå¸ˆ)
    """
    stop_tokens = set()
    
    # 0. åŸºç¡€è‡ªç§°ä¸é«˜é¢‘å™ªéŸ³
    self_refs = {'æœ¬æ ¡', 'æœ¬é™¢', 'æˆ‘æ ¡', 'æˆ‘é™¢', 'å®˜å¾®', 'å®˜å·', 'å­¦æ ¡', 'å­¦é™¢', 'å®˜æ–¹'}
    stop_tokens.update(self_refs)

    # å¸¸è§åç¼€ï¼Œç”¨äºæå–æ ¸å¿ƒè¯ (æ¸…åå¤§å­¦ -> æ¸…å)
    suffixes = ['å¤§å­¦', 'å­¦é™¢', 'å­¦æ ¡', 'ä¸­å­¦', 'å°å­¦', 'å¹¼å„¿å›­', 'å®˜æ–¹', 'å…¬ä¼—å·', 'æ•™è‚²', 'é›†å›¢', 'å›½é™…']
    
    for name in name_list:
        # 1. åŸå§‹å…¨å
        stop_tokens.add(name)
        
        # 2. ç»“å·´åˆ†è¯ (ä¾‹å¦‚ 'é‡åº†å¾·æ™®å¤–å›½è¯­å­¦æ ¡' -> 'é‡åº†', 'å¾·æ™®', 'å¤–å›½è¯­', 'å­¦æ ¡')
        tokens = jieba.lcut(name)
        for t in tokens:
            if len(t) > 1:
                stop_tokens.add(t)
                
        # 3. æ™ºèƒ½å»åç¼€ (ä¾‹å¦‚ 'æ¸…åå¤§å­¦' -> 'æ¸…å')
        core_name = name
        for suffix in suffixes:
            core_name = core_name.replace(suffix, "")
        if len(core_name) >= 2:
            stop_tokens.add(core_name)
            
        # 4. è¡Œä¸šç‰¹å®šç®€ç§°è§„åˆ™
        if len(name) >= 3:
            first_char = name[0] # å–é¦–å­— (å¦‚ 'å—', 'åŒ—', 'æ·±')
            
            # è§„åˆ™A: "Xå¤§" (åŒ—äº¬å¤§å­¦ -> åŒ—å¤§)
            if name.endswith("å¤§å­¦"):
                stop_tokens.add(first_char + "å¤§")
            
            # è§„åˆ™B: "Xå¤–" (å—äº¬å¤–å›½è¯­å­¦æ ¡ -> å—å¤–)
            if "å¤–å›½è¯­" in name:
                stop_tokens.add(first_char + "å¤–")
                
            # è§„åˆ™C: "Xä¸­" (æ·±åœ³ä¸­å­¦ -> æ·±ä¸­)
            if "ä¸­å­¦" in name and "å¤§å­¦" not in name:
                stop_tokens.add(first_char + "ä¸­")
                
            # è§„åˆ™D: "Xå¸ˆ" / "Xå¸ˆå¤§" (åå—å¸ˆèŒƒå¤§å­¦ -> åå¸ˆ)
            if "å¸ˆèŒƒ" in name:
                stop_tokens.add(first_char + "å¸ˆ")
                stop_tokens.add(first_char + "å¸ˆå¤§")
                
            # è§„åˆ™E: "Xé™„" (é™„å±)
            if "é™„å±" in name:
                stop_tokens.add(first_char + "é™„")

    return stop_tokens

def generate_wordcloud_img(text_data, exclude_words=None):
    """
    ç”Ÿæˆè¯äº‘å›¾ç‰‡å¯¹è±¡
    exclude_words: éœ€è¦é¢å¤–å±è”½çš„è¯åˆ—è¡¨
    """
    if not text_data:
        return None, []
        
    f_path = get_font_path()
    words = jieba.lcut(text_data)
    
    # åŸºç¡€é€šç”¨å±è”½è¯ (ä¿ç•™äº† ç”¨æˆ·å…³å¿ƒçš„ "å®¶é•¿", "å­¦ç”Ÿ" ç­‰è¯)
    base_stops = set([
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'åœ¨', 'ä¸º', 'å¯¹', 'ç­‰', 'ç¯‡', 
        'å¾®', 'ä¿¡', 'å·', 'æœˆ', 'æ—¥', 'å¹´', 'æœ‰', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£',
        'æˆ‘ä»¬', 'å›¾ç‰‡', 'æ¥æº', 'åŸæ ‡', 'é¢˜', 'å…¬ä¼—', 'ç‚¹å‡»', 'é˜…è¯»', 'åŸæ–‡', 'ä¸‹æ–¹', 'å…³æ³¨',
        'å±•å¼€', 'å…¨æ–‡', 'è§†é¢‘', 'åˆ†äº«', 'æ”¶è—', 'ç‚¹èµ', 'åœ¨çœ‹', 'æ‰«ç ', 'è¯†åˆ«', 'äºŒç»´ç ',
        'å®˜æ–¹', 'å¹³å°', 'å‘å¸ƒ', 'èµ„è®¯', 'æœåŠ¡', 'æŸ¥çœ‹', 'æ›´å¤š', 'å›å¤', 'å…³é”®å­—',
        'å­¦æ ¡', 'æ•™è‚²', 'å­¦é™¢', 'å¤§å­¦', 'ä¸­å­¦', 'å°å­¦', 'å¹¼å„¿å›­', 'å›½é™…', 'å¤–å›½è¯­', 'æ ¡åŒº' 
    ])
    
    if exclude_words:
        base_stops.update(exclude_words)
    
    filtered_words = [w for w in words if len(w) > 1 and w not in base_stops]
    space_split_text = " ".join(filtered_words)
    
    if not space_split_text.strip():
        return None, []

    try:
        wc = WordCloud(
            font_path=f_path,
            width=1000, 
            height=500,
            background_color='white',
            max_words=150,
            colormap='viridis',
            prefer_horizontal=0.9
        ).generate(space_split_text)
        return wc, filtered_words
    except Exception as e:
        print(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
        return None, []

# ==========================================
# AI åˆ†ææ¨¡å— (SiliconFlow)
# ==========================================

def call_ai_analysis(data_payload, mode="global"):
    """
    è°ƒç”¨ SiliconFlow API è¿›è¡Œ AI åˆ†æ
    mode: "global" (å…¨ç½‘å¯¹æ¯”) or "single" (å•å·åˆ†æ)
    """
    api_key = st.secrets.get("SILICONFLOW_API_KEY", "sk-lezqyzzxlcnarawzhmyddltuclijckeufnzzktmkizfslcje")
    
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    if mode == "global":
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•™è‚²è¡Œä¸šæ–°åª’ä½“æ•°æ®åˆ†æä¸“å®¶ã€‚
ç”¨æˆ·å°†æä¾›ä¸€ä»½JSONæ ¼å¼çš„æ±‡æ€»æ•°æ®ï¼ŒåŒ…å«å¤šä¸ªå…¬ä¼—å·åœ¨è¿‘æœŸçš„å‘æ–‡ç»Ÿè®¡ã€æ ‡é¢˜åˆ—è¡¨åŠæå–çš„é«˜é¢‘çƒ­è¯ã€‚
è¯·æ’°å†™ä¸€ä»½ã€æ·±åº¦èˆ†æƒ…å¯¹æ¯”åˆ†ææŠ¥å‘Šã€‘ã€‚

æŠ¥å‘Šæ ¸å¿ƒç»´åº¦ï¼š
1. **æ ¸å¿ƒè®®é¢˜æ¦‚è§ˆ**ï¼šåˆ†æå„å…¬ä¼—å·è¿‘æœŸå…³æ³¨çš„é‡ç‚¹è¯é¢˜ã€‚
2. **æ´»è·ƒåº¦ä¸ç­–ç•¥å¯¹æ¯”**ï¼šå¯¹æ¯”å„è´¦å·çš„å‘æ–‡é¢‘ç‡å’Œè¿è¥æ´»è·ƒåº¦ã€‚
3. **å†…å®¹é£æ ¼æ´å¯Ÿ**ï¼šåˆ†æå„è´¦å·çš„è¡Œæ–‡é£æ ¼ï¼ˆå¦‚æ ‡é¢˜å…šã€å­¦æœ¯ä¸¥è°¨ã€äº²æ°‘ç­‰ï¼‰ã€‚
4. **æ€»ç»“ä¸å»ºè®®**ï¼šç»™å‡ºä¼˜åŒ–å»ºè®®ã€‚

è¯·åŠ¡å¿…åœ¨å›ç­”å¼€å¤´å±•ç¤ºä½ çš„ã€æ€è€ƒè¿‡ç¨‹ã€‘(Reasoning Content)ï¼Œç„¶åå†è¾“å‡ºæœ€ç»ˆæŠ¥å‘Šã€‚"""
    else:
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•™è‚²è¡Œä¸šæ–°åª’ä½“è¿è¥é¡¾é—®ã€‚
ç”¨æˆ·å°†æä¾›ã€å•ä¸ªå…¬ä¼—å·ã€‘è¿‘æœŸçš„å‘æ–‡æ•°æ®ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€çƒ­è¯ï¼‰ã€‚
è¯·æ’°å†™ä¸€ä»½ã€å•è´¦å·è¿è¥è¯Šæ–­æŠ¥å‘Šã€‘ã€‚

æŠ¥å‘Šæ ¸å¿ƒç»´åº¦ï¼š
1. **äººè®¾ä¸é£æ ¼åˆ†æ**ï¼šåŸºäºæ ‡é¢˜å’Œæ‘˜è¦ï¼Œåˆ†æè¯¥è´¦å·è¯•å›¾æ‰“é€ ä»€ä¹ˆæ ·çš„äººè®¾ï¼Ÿ
2. **å†…å®¹åå¥½**ï¼šä»–ä»¬æœ€å–œæ¬¢å‘ä»€ä¹ˆç±»å‹çš„å†…å®¹ï¼Ÿ
3. **æ”¹è¿›å»ºè®®**ï¼šæœ‰å“ªäº›æ ‡é¢˜å¯ä»¥ä¼˜åŒ–ï¼Ÿæœ‰å“ªäº›çƒ­ç‚¹å¯ä»¥ç»“åˆï¼Ÿ

è¯·åŠ¡å¿…åœ¨å›ç­”å¼€å¤´å±•ç¤ºä½ çš„ã€æ€è€ƒè¿‡ç¨‹ã€‘(Reasoning Content)ï¼Œç„¶åå†è¾“å‡ºæœ€ç»ˆæŠ¥å‘Šã€‚"""

    data_json = json.dumps(data_payload, ensure_ascii=False, indent=2)
    
    payload = {
        "model": "moonshotai/Kimi-K2-Thinking", 
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¿™æ˜¯æ•°æ®ï¼š\n{data_json}"}
        ],
        "stream": False,
        "temperature": 0.7
    }
    
    try:
        # timeout è®¾ç½®ä¸º 300ç§’ (5åˆ†é’Ÿ)ï¼Œé˜²æ­¢æ¨ç†æ¨¡å‹è¶…æ—¶
        response = requests.post(url, headers=headers, json=payload, timeout=300)
        if response.status_code == 200:
            res_json = response.json()
            choice = res_json.get('choices', [{}])[0]
            message = choice.get('message', {})
            content = message.get('content', '')
            # å…¼å®¹ä¸åŒæ¨¡å‹çš„ reasoning å­—æ®µä½ç½®
            reasoning = message.get('reasoning_content', '') 
            return True, content, reasoning
        else:
            return False, f"API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}", ""
    except Exception as e:
        return False, f"å‘ç”Ÿé”™è¯¯: {str(e)}", ""

def prepare_global_ai_data(df):
    """å‡†å¤‡å…¨ç½‘åˆ†æçš„æ•°æ®"""
    summary_data = []
    # æ™ºèƒ½å±è”½è¯
    all_accounts = df['account_name'].unique()
    stop_tokens = get_smart_stopwords(all_accounts)
    base_stops = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'æœ‰', 'ç­‰', 'ä¸º', 'ä¹‹', 'ä¸', 'åŠ', 'ä»¥', 'å¾®', 'ä¿¡', 'å…¬ä¼—', 'å·', 'æ‰«ç ', 'äºŒç»´ç ', 'å…³æ³¨', 'é˜…è¯»', 'åŸæ–‡', 'ç‚¹å‡»', 'æŸ¥çœ‹', 'æ›´å¤š', 'æ¥æº', 'å›¾ç‰‡', 'è§†é¢‘', 'å±•å¼€', 'å…¨æ–‡'}
    stop_tokens.update(base_stops)

    for account_name in all_accounts:
        sub_df = df[df['account_name'] == account_name]
        text_content = " ".join(sub_df['title'].tolist() + sub_df['digest'].fillna("").tolist())
        words = jieba.lcut(text_content)
        words = [w for w in words if len(w) > 1 and w not in stop_tokens]
        top_keywords = [w for w, c in collections.Counter(words).most_common(10)]
        
        summary_data.append({
            "å…¬ä¼—å·å": account_name,
            "å‘æ–‡æ•°": len(sub_df),
            "æ–‡ç« æ ‡é¢˜": sub_df['title'].tolist(),
            "é«˜é¢‘çƒ­è¯": top_keywords
        })
    return summary_data

def prepare_single_ai_data(df, account_name):
    """å‡†å¤‡å•å·åˆ†æçš„æ•°æ®"""
    sub_df = df[df['account_name'] == account_name]
    return {
        "å…¬ä¼—å·å": account_name,
        "å‘æ–‡æ•°": len(sub_df),
        "æœ€æ–°å‘å¸ƒ": str(sub_df['å‘å¸ƒæ—¶é—´'].max()),
        "æ–‡ç« åˆ—è¡¨": [
            {"æ ‡é¢˜": row['title'], "æ‘˜è¦": row['digest']} 
            for _, row in sub_df.iterrows()
        ]
    }

# ==========================================
# çˆ¬è™«ç±»
# ==========================================

class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def check_auth(self):
        url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": "test", "begin": "0", "count": "1"
        }
        try:
            res = self.session.get(url, params=params)
            data = res.json()
            if "base_resp" in data and data["base_resp"]["ret"] != 0:
                return False, f"Token å¤±æ•ˆæˆ– Cookie è¿‡æœŸ: {data['base_resp']}"
            return True, "éªŒè¯é€šè¿‡"
        except:
            return False, "ç½‘ç»œè¿æ¥å¼‚å¸¸"

    def search_account(self, query):
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params, timeout=10)
            data = res.json()
            return data.get("list", [])
        except:
            return []

    def fetch_article_list(self, fakeid, pages=1):
        all_articles = []
        for page in range(pages):
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params, timeout=10)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "copyright_type": item.get("copyright_type", 0)
                        })
                else:
                    break
                time.sleep(random.uniform(0.5, 1.5))
            except:
                break
        return all_articles

    def fetch_content(self, url):
        try:
            res = self.session.get(url, timeout=15)
            soup = BeautifulSoup(res.text, "html.parser")
            content_div = soup.find("div", id="js_content")
            
            if content_div:
                final_html = clean_wechat_html(str(soup))
                plain_text = content_div.get_text(strip=True)
            else:
                final_html = clean_wechat_html(res.text)
                plain_text = ""
            
            author_tag = soup.find("strong", {"class": "profile_nickname"}) or soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            return final_html, author, plain_text
        except Exception:
            return "<div>è·å–å¤±è´¥</div>", "è·å–å¤±è´¥", ""

# ==========================================
# è‡ªåŠ¨åŒ–ç™»å½•
# ==========================================

def force_install_chromium():
    try:
        cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except:
        return False

def auto_login_browser():
    status_box = st.empty()
    qr_box = st.empty()
    token = None
    cookie_str = None

    status_box.info("ğŸš€ æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–å¼•æ“ (é«˜æ¸…äº‘ç«¯æ¨¡å¼)...")

    try:
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
            except Exception as e:
                if "Executable doesn't exist" in str(e):
                    status_box.warning("âš™ï¸ æ­£åœ¨å®‰è£…æµè§ˆå™¨å†…æ ¸...")
                    if force_install_chromium():
                         status_box.success("âœ… å®‰è£…æˆåŠŸï¼é‡è¯•ä¸­...")
                         browser = p.chromium.launch(headless=True)
                    else:
                         return None, None
                else:
                    raise e

            context = browser.new_context(viewport={'width': 1920, 'height': 1080})
            page = context.new_page()

            status_box.info("ğŸ”— æ­£åœ¨åŠ è½½å¾®ä¿¡ç™»å½•é¡µ...")
            page.goto("https://mp.weixin.qq.com/")
            
            try:
                page.wait_for_selector(".login__type__container__scan", timeout=15000)
            except:
                pass 

            status_box.warning("ğŸ“± è¯·ä½¿ç”¨æ‰‹æœºå¾®ä¿¡æ‰«ç  (äºŒç»´ç å·²æ”¾å¤§):")
            
            max_wait = 120
            for i in range(max_wait):
                try:
                    if page.is_closed(): return None, None
                    current_url = page.url
                except: return None, None

                if i % 1.5 == 0 and "token=" not in current_url:
                    try:
                        qr_elem = page.locator(".login__type__container__scan")
                        if qr_elem.count() > 0:
                            screenshot_bytes = qr_elem.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å®æ—¶ç”»é¢)", width=400)
                        else:
                            screenshot_bytes = page.screenshot()
                            qr_box.image(screenshot_bytes, caption="ğŸ“¸ è¯·æ‰«ç  (å…¨å±å¤‡ç”¨)", width=600)
                    except:
                        pass

                if "token=" in current_url:
                    qr_box.empty()
                    status_box.success(f"âœ… ç™»å½•æˆåŠŸï¼")
                    
                    parsed = urlparse(current_url)
                    token = parse_qs(parsed.query).get("token", [""])[0]
                    cookies = context.cookies()
                    cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                    time.sleep(1)
                    break
                else:
                    time.sleep(1)
            
            browser.close()
            
    except Exception as e:
        status_box.error(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        return None, None

    return token, cookie_str

# ==========================================
# Streamlit ä¸»ç•Œé¢
# ==========================================

if 'wx_token' not in st.session_state: st.session_state['wx_token'] = ''
if 'wx_cookie' not in st.session_state: st.session_state['wx_cookie'] = ''
if 'all_data' not in st.session_state: st.session_state['all_data'] = None

with st.sidebar:
    st.title("ğŸ“ å…¬ä¼—å·èˆ†æƒ…åˆ†æ Pro")
    st.caption("Playwright Â· è¯äº‘ Â· Kimiæ·±åº¦æ€è€ƒ")
    st.markdown("---")
    
    if st.button("ğŸ“¢ 1. æ‰«ç è·å–æƒé™", type="primary", use_container_width=True):
        token, cookie = auto_login_browser()
        if token and cookie:
            st.session_state['wx_token'] = token
            st.session_state['wx_cookie'] = cookie
            st.success("æƒé™è·å–æˆåŠŸï¼")
            time.sleep(1)
            st.rerun()

    with st.expander("ğŸ”‘ å‡­è¯ç®¡ç†", expanded=True):
        token_input = st.text_input("Token", value=st.session_state['wx_token'])
        cookie_input = st.text_area("Cookie", value=st.session_state['wx_cookie'], height=100)
        
        if token_input != st.session_state['wx_token']: st.session_state['wx_token'] = token_input
        if cookie_input != st.session_state['wx_cookie']: st.session_state['wx_cookie'] = cookie_input

    st.markdown("---")
    targets_input = st.text_area(
        "2. è¾“å…¥å…¬ä¼—å·åç§°", 
        placeholder="æ”¯æŒä¸­æ–‡é€—å·ã€é¡¿å·ã€ç©ºæ ¼æˆ–æ¢è¡Œåˆ†éš”\nä¾‹å¦‚ï¼š\næ¸…åå¤§å­¦ã€åŒ—äº¬å¤§å­¦ï¼Œå¤æ—¦å¤§å­¦",
        height=150
    )
    
    page_count = st.slider("æ¯ä¸ªå·æŠ“å–é¡µæ•° (æ¯é¡µ5ç¯‡)", 1, 5, 2)
    run_btn = st.button("ğŸš€ 3. å¼€å§‹åˆ†æ", use_container_width=True)

# --- ä¸»é€»è¾‘åŒº ---

if run_btn:
    if not token_input or not cookie_input:
        st.error("è¯·å…ˆè·å– Token å’Œ Cookieï¼")
        st.stop()
    if not targets_input.strip():
        st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·ï¼")
        st.stop()
        
    target_list = re.split(r'[,\s\nï¼Œã€]+', targets_input.strip())
    target_list = [t for t in target_list if t]
    
    crawler = WechatCrawler(token_input, cookie_input)
    
    all_results = []
    status_container = st.status("æ­£åœ¨è¿›è¡Œæ•°æ®é‡‡é›†...", expanded=True)
    progress_bar = st.progress(0)
    
    with status_container:
        if not crawler.check_auth()[0]:
            st.error("æƒé™éªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°æ‰«ç ï¼")
            st.stop()
            
        total_targets = len(target_list)
        for i, target_name in enumerate(target_list):
            st.write(f"ğŸ”„ [{i+1}/{total_targets}] åˆ†æ: **{target_name}** ...")
            
            accounts = crawler.search_account(target_name)
            if not accounts:
                st.warning(f"âš ï¸ æœªæ‰¾åˆ°: {target_name}ï¼Œè·³è¿‡")
                continue
            
            target_account = accounts[0]
            fakeid = target_account['fakeid']
            real_nickname = target_account['nickname']
            
            articles = crawler.fetch_article_list(fakeid, pages=page_count)
            
            if articles:
                st.write(f"   - æŠ“å–æ­£æ–‡ ({len(articles)}ç¯‡)...")
                for art in articles:
                    html_content, author, plain_text = crawler.fetch_content(art['link'])
                    art['content_html'] = html_content
                    art['plain_text'] = plain_text
                    art['author'] = author
                    art['account_name'] = real_nickname
                    time.sleep(0.5)

            all_results.extend(articles)
            progress_bar.progress((i + 1) / total_targets)
            time.sleep(random.uniform(1.0, 2.0))
            
        status_container.update(label="âœ… é‡‡é›†ä¸åˆ†æå®Œæˆï¼", state="complete")
    
    if all_results:
        df = pd.DataFrame(all_results)
        df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['create_time'], unit='s')
        df['å‘å¸ƒæ—¥æœŸ'] = df['å‘å¸ƒæ—¶é—´'].dt.date
        df['ç±»å‹'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
        st.session_state['all_data'] = df
    else:
        st.warning("æœªé‡‡é›†åˆ°æ•°æ®ã€‚")

# --- åˆ†æçœ‹æ¿ ---

if st.session_state['all_data'] is not None:
    df = st.session_state['all_data']
    
    st.divider()
    st.title("ğŸ“Š å…¬ä¼—å·æ–°åª’ä½“å¤§æ•°æ®çœ‹æ¿")
    
    # 4ä¸ª Tab
    tab_global_1, tab_global_2, tab_global_3, tab_ai = st.tabs(["â˜ï¸ ç»¼åˆè¯äº‘", "ğŸ† å½±å“åŠ›æ’è¡Œ", "ğŸ“ˆ å‘æ–‡è¶‹åŠ¿", "ğŸ¤– å…¨ç½‘ AI æŠ¥å‘Š"])
    
    # å…¨ç½‘æ™ºèƒ½å±è”½è¯
    all_accounts = df['account_name'].unique()
    global_stop_words = get_smart_stopwords(all_accounts)
    
    with tab_global_1:
        col_g1, col_g2 = st.columns(2)
        with col_g1:
            st.subheader("å…¨ç½‘Â·æ ‡é¢˜è¯äº‘")
            all_titles = " ".join(df['title'].tolist())
            wc_title, _ = generate_wordcloud_img(all_titles, exclude_words=global_stop_words)
            if wc_title:
                st.image(wc_title.to_array(), use_container_width=True)
            else:
                st.info("æ•°æ®ä¸è¶³")
                
        with col_g2:
            st.subheader("å…¨ç½‘Â·å†…å®¹è¯äº‘")
            all_contents = " ".join(df['plain_text'].fillna("").tolist())
            wc_content, words_list = generate_wordcloud_img(all_contents, exclude_words=global_stop_words)
            if wc_content:
                st.image(wc_content.to_array(), use_container_width=True)
            
            if words_list:
                st.markdown("**ğŸ”¥ å…¨ç½‘ TOP 10 çƒ­è¯:**")
                counts = collections.Counter(words_list)
                top10 = counts.most_common(10)
                
                # UI ç¾åŒ–ï¼šä½¿ç”¨ Progress Bar å±•ç¤º Top 10
                top10_data = pd.DataFrame(top10, columns=['å…³é”®è¯', 'é¢‘æ¬¡'])
                max_count = int(top10_data['é¢‘æ¬¡'].max()) if not top10_data.empty else 1
                
                st.dataframe(
                    top10_data,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "å…³é”®è¯": st.column_config.TextColumn("ğŸ”¥ å…³é”®è¯", width="medium"),
                        "é¢‘æ¬¡": st.column_config.ProgressColumn(
                            "çƒ­åº¦æŒ‡æ•°",
                            format="%d",
                            min_value=0,
                            max_value=max_count,
                            width="large"
                        )
                    }
                )

    with tab_global_2:
        st.caption("æ³¨ï¼šæ•°æ®åŸºäºæœ¬æ¬¡æŠ“å–çš„æ ·æœ¬è®¡ç®—ã€‚")
        col_r1, col_r2 = st.columns(2)
        now = pd.Timestamp.now()
        
        with col_r1:
            st.markdown("#### ğŸ“… æœ¬å‘¨å‘æ–‡æ¦œ")
            week_df = df[df['å‘å¸ƒæ—¶é—´'] > (now - pd.Timedelta(days=7))]
            if not week_df.empty:
                week_rank = week_df['account_name'].value_counts().reset_index()
                week_rank.columns = ['å…¬ä¼—å·', 'å‘æ–‡æ•°']
                st.dataframe(
                    week_rank, 
                    use_container_width=True, 
                    hide_index=True,
                    column_config={
                        "å‘æ–‡æ•°": st.column_config.NumberColumn(
                            "æœ¬å‘¨å‘æ–‡",
                            format="%d ç¯‡"
                        )
                    }
                )
            else:
                st.info("æœ¬å‘¨æ— æ•°æ®")
                
        with col_r2:
            st.markdown("#### ğŸ—“ï¸ æœ¬æœˆå‘æ–‡æ¦œ")
            month_df = df[df['å‘å¸ƒæ—¶é—´'] > (now - pd.Timedelta(days=30))]
            if not month_df.empty:
                month_rank = month_df['account_name'].value_counts().reset_index()
                month_rank.columns = ['å…¬ä¼—å·', 'å‘æ–‡æ•°']
                st.dataframe(
                    month_rank, 
                    use_container_width=True, 
                    hide_index=True,
                    column_config={
                        "å‘æ–‡æ•°": st.column_config.NumberColumn(
                            "æœ¬æœˆå‘æ–‡",
                            format="%d ç¯‡"
                        )
                    }
                )
            else:
                st.info("æœ¬æœˆæ— æ•°æ®")

    with tab_global_3:
        st.subheader("å…¨ç½‘å‘å¸ƒæ—¶é—´åˆ†å¸ƒ")
        date_counts = df.groupby('å‘å¸ƒæ—¥æœŸ').size()
        st.line_chart(date_counts)

    # --- å…¨ç½‘ AI åˆ†æ ---
    with tab_ai:
        st.subheader("ğŸ¤– Kimi-K2-Thinking æ·±åº¦èˆ†æƒ…æŠ¥å‘Š (å…¨ç½‘ç‰ˆ)")
        st.info("AI å°†å¯¹æ¯”åˆ†ææ‰€æœ‰æŠ“å–çš„å…¬ä¼—å·æ•°æ®ã€‚")
        
        if st.button("ğŸ§  å¼€å§‹å…¨ç½‘ AI åˆ†æ", type="primary", key="btn_global_ai"):
            with st.spinner("AI æ­£åœ¨æ·±åº¦æ€è€ƒä¸­ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)..."):
                ai_data = prepare_global_ai_data(df)
                success, report, reasoning = call_ai_analysis(ai_data, mode="global")
            
            if success:
                # æ€è€ƒè¿‡ç¨‹å¯è§†åŒ–
                if reasoning:
                    with st.chat_message("assistant", avatar="ğŸ§ "):
                        st.markdown("**AI æ€è€ƒè¿‡ç¨‹ (Reasoning):**")
                        st.caption(reasoning)
                    st.divider()
                
                # æœ€ç»ˆæŠ¥å‘Š
                st.markdown(report)
                st.success("åˆ†æå®Œæˆï¼")
            else:
                st.error(report)

    st.markdown("---")

    # 2. ä¸ªä½“ç”»åƒ
    st.header("2. å•å·æ·±åº¦ç”»åƒ (Single Account)")
    
    account_list = df['account_name'].unique()
    selected_account = st.selectbox("ğŸ‘‰ é€‰æ‹©ä¸€ä¸ªå…¬ä¼—å·æŸ¥çœ‹è¯¦æƒ…:", account_list)
    
    if selected_account:
        sub_df = df[df['account_name'] == selected_account].copy()
        
        # é’ˆå¯¹å•ä¸ªè´¦å·ç”Ÿæˆç‰¹å®šçš„æ™ºèƒ½å±è”½è¯
        account_stop_words = get_smart_stopwords([selected_account])
        
        c1, c2, c3 = st.columns(3)
        c1.metric("æ€»å‘æ–‡æ•°", len(sub_df))
        c2.metric("åŸåˆ›æ¯”ä¾‹", f"{len(sub_df[sub_df['ç±»å‹']=='åŸåˆ›']) / len(sub_df) * 100:.1f}%" if len(sub_df)>0 else "0%")
        c3.metric("æœ€æ–°å‘å¸ƒ", str(sub_df['å‘å¸ƒæ—¥æœŸ'].max()))
        
        tab_s1, tab_s2, tab_s3 = st.tabs(["ğŸ“Š æ•°æ®åˆ†æ", "ğŸ“° æ–‡ç« åˆ—è¡¨ä¸é˜…è¯»", "ğŸ§  AI å•å·è¯Šæ–­"])
        
        with tab_s1:
            sc1, sc2 = st.columns(2)
            with sc1:
                st.markdown("**æ ‡é¢˜è¯äº‘**")
                s_titles = " ".join(sub_df['title'].tolist())
                s_wc_t, _ = generate_wordcloud_img(s_titles, exclude_words=account_stop_words)
                if s_wc_t: st.image(s_wc_t.to_array(), use_container_width=True)
                
            with sc2:
                st.markdown("**å†…å®¹è¯äº‘**")
                s_content = " ".join(sub_df['plain_text'].fillna("").tolist())
                s_wc_c, s_words = generate_wordcloud_img(s_content, exclude_words=account_stop_words)
                if s_wc_c: 
                    st.image(s_wc_c.to_array(), use_container_width=True)
                    if s_words:
                        st.markdown("**ğŸ”¥ è´¦å· TOP 10 çƒ­è¯:**")
                        s_counts = collections.Counter(s_words)
                        s_top10 = s_counts.most_common(10)
                        
                        s_top10_df = pd.DataFrame(s_top10, columns=['å…³é”®è¯', 'é¢‘æ¬¡'])
                        s_max_count = int(s_top10_df['é¢‘æ¬¡'].max()) if not s_top10_df.empty else 1
                        
                        st.dataframe(
                            s_top10_df,
                            use_container_width=True,
                            hide_index=True,
                            column_config={
                                "å…³é”®è¯": st.column_config.TextColumn("å…³é”®è¯", width="medium"),
                                "é¢‘æ¬¡": st.column_config.ProgressColumn(
                                    "é¢‘æ¬¡",
                                    format="%d",
                                    min_value=0,
                                    max_value=s_max_count,
                                    width="large"
                                )
                            }
                        )

        with tab_s2:
            col_list, col_read = st.columns([1, 2])
            with col_list:
                st.markdown("##### æ–‡ç« åˆ—è¡¨")
                sub_df['label'] = sub_df.apply(lambda x: f"{x['å‘å¸ƒæ—¥æœŸ']} | {x['title']}", axis=1)
                selected_article_label = st.radio(
                    "ç‚¹å‡»é€‰æ‹©æ–‡ç« é˜…è¯»:",
                    sub_df['label'].tolist(),
                    label_visibility="collapsed"
                )
            
            with col_read:
                if selected_article_label:
                    read_art = sub_df[sub_df['label'] == selected_article_label].iloc[0]
                    st.markdown(f"#### {read_art['title']}")
                    st.caption(f"âœï¸ {read_art['author']}  |  ğŸ•’ {read_art['å‘å¸ƒæ—¶é—´']}")
                    st.divider()
                    if 'content_html' in read_art and read_art['content_html']:
                        st.components.v1.html(read_art['content_html'], height=800, scrolling=True)
                    else:
                        st.warning("æ­£æ–‡å†…å®¹ä¸ºç©º")
                        
        with tab_s3:
            st.subheader(f"ğŸ§  {selected_account} - è¿è¥è¯Šæ–­æŠ¥å‘Š")
            if st.button("å¼€å§‹å•å·è¯Šæ–­", type="primary", key=f"btn_single_{selected_account}"):
                with st.spinner("AI æ­£åœ¨è¯Šæ–­è¯¥è´¦å· (è¯·ç¨å€™)..."):
                    single_ai_data = prepare_single_ai_data(df, selected_account)
                    success, report, reasoning = call_ai_analysis(single_ai_data, mode="single")
                
                if success:
                    if reasoning:
                        with st.chat_message("assistant", avatar="ğŸ§ "):
                            st.markdown("**AI æ€è€ƒè¿‡ç¨‹:**")
                            st.caption(reasoning)
                        st.divider()
                    st.markdown(report)
                else:
                    st.error(report)

else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ æ“ä½œï¼šæ‰«ç  -> è¾“å…¥å…¬ä¼—å· -> å¼€å§‹åˆ†æ")

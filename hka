import streamlit as st
import pandas as pd
import requests
import time
import random
import plotly.express as px
import plotly.graph_objects as go
import datetime
from bs4 import BeautifulSoup
from collections import Counter
import jieba
import jieba.analyse

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="WeChat Insight Pro (Reader Mode)",
    page_icon="ğŸ“–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ ¸å¿ƒçˆ¬è™«é€»è¾‘ ---

class WechatCrawler:
    def __init__(self, token, cookie):
        self.base_url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
            "Cookie": cookie
        }
        self.token = token
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search_account(self, query):
        """æœç´¢å…¬ä¼—å·è·å–fakeid"""
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
        params = {
            "action": "search_biz", "token": self.token, "lang": "zh_CN",
            "f": "json", "ajax": "1", "query": query, "begin": "0", "count": "5",
        }
        try:
            res = self.session.get(search_url, params=params)
            data = res.json()
            return data.get("list", [])
        except Exception as e:
            st.error(f"æœç´¢è¯·æ±‚å¼‚å¸¸: {e}")
            return []

    def fetch_article_list(self, fakeid, pages=3):
        """è·å–æ–‡ç« åˆ—è¡¨å…ƒæ•°æ®"""
        all_articles = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for page in range(pages):
            status_text.text(f"ğŸ“¡ æ­£åœ¨æ‰«æåˆ—è¡¨ç¬¬ {page + 1}/{pages} é¡µ...")
            params = {
                "token": self.token, "lang": "zh_CN", "f": "json", "ajax": "1",
                "action": "list_ex", "fakeid": fakeid, "query": "",
                "begin": str(page * 5), "count": "5", "type": "9",
            }
            try:
                res = self.session.get(self.base_url, params=params)
                data = res.json()
                if "app_msg_list" in data:
                    for item in data["app_msg_list"]:
                        all_articles.append({
                            "aid": item.get("aid"),
                            "title": item.get("title"),
                            "digest": item.get("digest"),
                            "link": item.get("link"),
                            "create_time": item.get("create_time"),
                            "cover": item.get("cover"),
                            "item_idx": item.get("item_idx", 1), # 1ä¸ºå¤´æ¡ï¼Œ2ä¸ºæ¬¡æ¡
                            "copyright_type": item.get("copyright_type", 0) # 1åŸåˆ›
                        })
                else:
                    break
                progress_bar.progress((page + 1) / pages)
                time.sleep(random.uniform(1.5, 3)) # åˆ—è¡¨é¡µè¯·æ±‚é—´éš”
            except:
                break
        
        progress_bar.empty()
        status_text.empty()
        return all_articles

    def fetch_article_content(self, url):
        """
        æ·±åº¦é‡‡é›†ï¼šè®¿é—®è¯¦æƒ…é¡µè·å–æ­£æ–‡ã€ä½œè€…ç­‰ä¿¡æ¯
        """
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, "lxml")
            
            # æå–æ­£æ–‡æ–‡æœ¬ (å»é™¤HTMLæ ‡ç­¾ï¼Œä¿ç•™æ®µè½ç»“æ„)
            content_div = soup.find("div", {"id": "js_content"})
            if content_div:
                # ç®€å•å¤„ç†ï¼šå°†pæ ‡ç­¾æ¢è¡Œï¼Œå¢å¼ºé˜…è¯»ä½“éªŒ
                for p in content_div.find_all('p'):
                    p.insert_after('\n')
                content_text = content_div.get_text().strip()
            else:
                content_text = ""
            
            # æå–ä½œè€…
            author_tag = soup.find("strong", {"class": "profile_nickname"}) # æ—§ç‰ˆ
            if not author_tag:
                author_tag = soup.find("a", {"id": "js_name"})
            author = author_tag.get_text().strip() if author_tag else "æœªçŸ¥"
            
            # æå–IPå±åœ°
            scripts = soup.find_all("script")
            ip_location = "IPæœªçŸ¥"
            for script in scripts:
                if script.string and "ip_wording" in script.string:
                    import re
                    match = re.search(r'ip_wording\s*=\s*\{\s*type\s*:\s*2\s*,\s*name\s*:\s*"(.*?)"', script.string)
                    if match:
                        ip_location = match.group(1)
                        break
            
            return content_text, author, ip_location
        except Exception:
            return "", "è·å–å¤±è´¥", "è·å–å¤±è´¥"

# --- æ•°æ®å¤„ç†å·¥å…· ---

def process_data(articles, crawler=None, fetch_details=False):
    if not articles:
        return pd.DataFrame()
    
    df = pd.DataFrame(articles)
    
    # æ—¶é—´å¤„ç†
    df['publish_time'] = pd.to_datetime(df['create_time'], unit='s')
    df['date'] = df['publish_time'].dt.date
    df['year_week'] = df['publish_time'].dt.strftime('%Y-ç¬¬%Wå‘¨')
    df['weekday'] = df['publish_time'].dt.weekday
    df['hour'] = df['publish_time'].dt.hour
    
    # æ ‡è¯†å¤„ç†
    df['position'] = df['item_idx'].apply(lambda x: 'å¤´æ¡' if x == 1 else f'æ¬¡æ¡({x})')
    df['is_original'] = df['copyright_type'].apply(lambda x: 'åŸåˆ›' if x == 1 else 'è½¬è½½')
    
    # æ·±åº¦é‡‡é›†
    if fetch_details and crawler:
        st.info("ğŸ¢ æ­£åœ¨æ·±åº¦é‡‡é›†å…¨æ–‡ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        details = []
        bar = st.progress(0)
        for idx, row in df.iterrows():
            content, author, ip = crawler.fetch_article_content(row['link'])
            details.append({
                'content': content,
                'author': author,
                'ip_location': ip
            })
            bar.progress((idx + 1) / len(df))
            time.sleep(random.uniform(0.5, 1.5)) # å¿…é¡»å»¶æ—¶
        
        detail_df = pd.DataFrame(details)
        df = pd.concat([df, detail_df], axis=1)
        bar.empty()
    else:
        df['content'] = ""
        df['author'] = "æœªé‡‡é›†"
        df['ip_location'] = "-"

    return df

def extract_keywords(df):
    """æå–æ ‡é¢˜å’Œæ­£æ–‡ä¸­çš„å…³é”®è¯"""
    text_corpus = "".join(df['title'].astype(str).tolist())
    if 'content' in df.columns and df['content'].any():
        # å¦‚æœé‡‡é›†äº†æ­£æ–‡ï¼Œæƒé‡ç¨å¾®ä½ä¸€ç‚¹åŠ å…¥è¯­æ–™
        content_corpus = "".join(df['content'].astype(str).tolist())
        text_corpus += content_corpus[:100000] # é™åˆ¶é•¿åº¦é˜²æ­¢è¿‡æ…¢
        
    keywords = jieba.analyse.extract_tags(text_corpus, topK=20, withWeight=True)
    return pd.DataFrame(keywords, columns=['word', 'weight'])

# --- ä¸»ç¨‹åºé€»è¾‘ ---

with st.sidebar:
    st.title("ğŸ“– å…¬ä¼—å·çƒ­ç‚¹é˜…è¯»å™¨")
    st.caption("çœŸå®æ•°æ® Â· å…³é”®è¯æŒ–æ˜ Â· æ²‰æµ¸é˜…è¯»")
    
    with st.expander("ğŸ”‘ å‡­è¯é…ç½® (å¿…å¡«)", expanded=True):
        wx_token = st.text_input("Token", help="URLä¸­çš„tokenå‚æ•°")
        wx_cookie = st.text_area("Cookie", help="F12è·å–çš„å®Œæ•´Cookie")
    
    st.divider()
    target_query = st.text_input("ğŸ” ç›®æ ‡å…¬ä¼—å·", placeholder="è¾“å…¥åç§°")
    
    col1, col2 = st.columns(2)
    with col1:
        scrape_pages = st.number_input("æŠ“å–é¡µæ•°", 1, 10, 2)
    with col2:
        # æ—¢ç„¶ç”¨æˆ·è¦è¯»å…¨æ–‡ï¼Œè¿™é‡Œé»˜è®¤ä¸º True æ¯”è¾ƒå¥½ï¼Œä½†ä¸ºäº†é˜²å°å·è¿˜æ˜¯ç•™é€‰é¡¹
        enable_details = st.checkbox("é‡‡é›†æ­£æ–‡", value=True, help="å¿…é¡»å‹¾é€‰æ‰èƒ½é˜…è¯»å…¨æ–‡")
        
    start_btn = st.button("ğŸš€ å¼€å§‹æŠ“å–", type="primary", use_container_width=True)

# --- ä¸»ç•Œé¢ ---

if start_btn and wx_token and wx_cookie and target_query:
    crawler = WechatCrawler(wx_token, wx_cookie)
    
    with st.status("æ­£åœ¨å»ºç«‹æ•°æ®è¿æ¥...", expanded=True) as status:
        status.write("ğŸ” å®šä½ç›®æ ‡è´¦å·...")
        accounts = crawler.search_account(target_query)
        if not accounts:
            status.update(label="æœªæ‰¾åˆ°è´¦å·ï¼Œè¯·æ£€æŸ¥Cookie", state="error")
            st.stop()
        
        target = accounts[0]
        status.write(f"âœ… é”å®š: {target['nickname']}")
        
        status.write("ğŸ“ƒ æ‹‰å–æ–‡ç« åˆ—è¡¨...")
        raw_list = crawler.fetch_article_list(target['fakeid'], pages=scrape_pages)
        
        if not raw_list:
            status.update(label="æœªè·å–åˆ°æ•°æ®", state="error")
            st.stop()

        status.write("ğŸ§¹ æ·±åº¦é‡‡é›†æ­£æ–‡å†…å®¹...")
        df_res = process_data(raw_list, crawler, fetch_details=enable_details)
        
        status.update(label="æ•°æ®å‡†å¤‡å°±ç»ª!", state="complete")
        
        st.session_state['data'] = df_res
        st.session_state['account'] = target['nickname']

# --- çœ‹æ¿å±•ç¤º ---

if 'data' in st.session_state:
    df = st.session_state['data']
    nickname = st.session_state['account']
    
    st.header(f"ğŸ“° {nickname} Â· æ·±åº¦é˜…è¯»çœ‹æ¿")
    
    # --- Tab åˆ†åŒº ---
    tab_read, tab_hot, tab_list = st.tabs(["ğŸ‘“ æ²‰æµ¸é˜…è¯»æ¨¡å¼", "ğŸ”¥ æ ¸å¿ƒçƒ­ç‚¹åˆ†æ", "ğŸ“‹ æ–‡ç« åˆ—è¡¨"])
    
    # 1. æ²‰æµ¸é˜…è¯»æ¨¡å¼
    with tab_read:
        if enable_details and 'content' in df.columns:
            # æ‹¼æ¥æ ‡é¢˜å’Œæ—¥æœŸä½œä¸ºé€‰é¡¹
            df['select_label'] = df['date'].astype(str) + " | " + df['title']
            selected_article_label = st.selectbox("é€‰æ‹©è¦é˜…è¯»çš„æ–‡ç« :", df['select_label'].tolist())
            
            # è·å–é€‰ä¸­æ–‡ç« æ•°æ®
            article = df[df['select_label'] == selected_article_label].iloc[0]
            
            with st.container():
                st.markdown(f"## {article['title']}")
                st.caption(f"ä½œè€…: {article['author']} | å‘å¸ƒæ—¶é—´: {article['publish_time']} | {article['is_original']} | IPå±åœ°: {article['ip_location']}")
                st.divider()
                
                # æ­£æ–‡å±•ç¤ºåŒº
                if article['content']:
                    st.markdown(article['content'].replace("\n", "\n\n")) # å¢åŠ Markdownæ¢è¡Œ
                else:
                    st.warning("æ­£æ–‡æœªé‡‡é›†ï¼Œè¯·ç¡®ä¿å‹¾é€‰ä¾§è¾¹æ çš„ã€é‡‡é›†æ­£æ–‡ã€‘å¹¶é‡æ–°æŠ“å–ã€‚")
                    st.markdown(f"[ç‚¹å‡»è·³è½¬åŸæ–‡é“¾æ¥]({article['link']})")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ å‹¾é€‰ã€é‡‡é›†æ­£æ–‡ã€‘ä»¥å¯ç”¨é˜…è¯»æ¨¡å¼ã€‚")

    # 2. æ ¸å¿ƒçƒ­ç‚¹åˆ†æ
    with tab_hot:
        st.subheader("è¯é¢‘çƒ­ç‚¹æŒ–æ˜")
        st.caption("åŸºäºæ–‡ç« æ ‡é¢˜å’Œæ­£æ–‡çš„TF-IDFç®—æ³•åˆ†æï¼ŒæŒ–æ˜è¯¥å…¬ä¼—å·è¿‘æœŸçš„æ ¸å¿ƒå…³æ³¨ç‚¹ã€‚")
        
        if not df.empty:
            keywords_df = extract_keywords(df)
            
            c1, c2 = st.columns([2, 1])
            with c1:
                fig = px.bar(keywords_df, x='weight', y='word', orientation='h', 
                             title="æ ¸å¿ƒçƒ­è¯ TOP 20", labels={'weight': 'çƒ­åº¦æƒé‡', 'word': 'å…³é”®è¯'},
                             color='weight', color_continuous_scale='Reds')
                fig.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
            with c2:
                st.write("ğŸ“‹ **çƒ­è¯åˆ—è¡¨**")
                st.dataframe(keywords_df, use_container_width=True)
        else:
            st.write("æš‚æ— æ•°æ®")

    # 3. æ–‡ç« åˆ—è¡¨
    with tab_list:
        st.dataframe(
            df[['title', 'date', 'author', 'is_original', 'link']],
            use_container_width=True,
            column_config={
                "link": st.column_config.LinkColumn("åŸæ–‡é“¾æ¥")
            }
        )

else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é…ç½®æŠ“å–å‚æ•°ã€‚ä¸ºäº†é˜…è¯»å…¨æ–‡ï¼Œè¯·åŠ¡å¿…å‹¾é€‰ã€é‡‡é›†æ­£æ–‡ã€‘ã€‚")
